{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "Xuo4VwU6t1qu",
        "outputId": "ca1bb780-feda-45a6-c12b-372b87e2192b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>day</th>\n",
              "      <th>country</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>hazard-category</th>\n",
              "      <th>product-category</th>\n",
              "      <th>hazard</th>\n",
              "      <th>product</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1994</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>us</td>\n",
              "      <td>Recall Notification: FSIS-024-94</td>\n",
              "      <td>Case Number: 024-94   \\n            Date Opene...</td>\n",
              "      <td>biological</td>\n",
              "      <td>meat, egg and dairy products</td>\n",
              "      <td>listeria monocytogenes</td>\n",
              "      <td>smoked sausage</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1994</td>\n",
              "      <td>3</td>\n",
              "      <td>10</td>\n",
              "      <td>us</td>\n",
              "      <td>Recall Notification: FSIS-033-94</td>\n",
              "      <td>Case Number: 033-94   \\n            Date Opene...</td>\n",
              "      <td>biological</td>\n",
              "      <td>meat, egg and dairy products</td>\n",
              "      <td>listeria spp</td>\n",
              "      <td>sausage</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1994</td>\n",
              "      <td>3</td>\n",
              "      <td>28</td>\n",
              "      <td>us</td>\n",
              "      <td>Recall Notification: FSIS-014-94</td>\n",
              "      <td>Case Number: 014-94   \\n            Date Opene...</td>\n",
              "      <td>biological</td>\n",
              "      <td>meat, egg and dairy products</td>\n",
              "      <td>listeria monocytogenes</td>\n",
              "      <td>ham slices</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1994</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>us</td>\n",
              "      <td>Recall Notification: FSIS-009-94</td>\n",
              "      <td>Case Number: 009-94   \\n            Date Opene...</td>\n",
              "      <td>foreign bodies</td>\n",
              "      <td>meat, egg and dairy products</td>\n",
              "      <td>plastic fragment</td>\n",
              "      <td>thermal processed pork meat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>1994</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>us</td>\n",
              "      <td>Recall Notification: FSIS-001-94</td>\n",
              "      <td>Case Number: 001-94   \\n            Date Opene...</td>\n",
              "      <td>foreign bodies</td>\n",
              "      <td>meat, egg and dairy products</td>\n",
              "      <td>plastic fragment</td>\n",
              "      <td>chicken breast</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  year  month  day country                             title  \\\n",
              "0           0  1994      1    7      us  Recall Notification: FSIS-024-94   \n",
              "1           1  1994      3   10      us  Recall Notification: FSIS-033-94   \n",
              "2           2  1994      3   28      us  Recall Notification: FSIS-014-94   \n",
              "3           3  1994      4    3      us  Recall Notification: FSIS-009-94   \n",
              "4           4  1994      7    1      us  Recall Notification: FSIS-001-94   \n",
              "\n",
              "                                                text hazard-category  \\\n",
              "0  Case Number: 024-94   \\n            Date Opene...      biological   \n",
              "1  Case Number: 033-94   \\n            Date Opene...      biological   \n",
              "2  Case Number: 014-94   \\n            Date Opene...      biological   \n",
              "3  Case Number: 009-94   \\n            Date Opene...  foreign bodies   \n",
              "4  Case Number: 001-94   \\n            Date Opene...  foreign bodies   \n",
              "\n",
              "               product-category                  hazard  \\\n",
              "0  meat, egg and dairy products  listeria monocytogenes   \n",
              "1  meat, egg and dairy products            listeria spp   \n",
              "2  meat, egg and dairy products  listeria monocytogenes   \n",
              "3  meat, egg and dairy products        plastic fragment   \n",
              "4  meat, egg and dairy products        plastic fragment   \n",
              "\n",
              "                       product  \n",
              "0               smoked sausage  \n",
              "1                      sausage  \n",
              "2                   ham slices  \n",
              "3  thermal processed pork meat  \n",
              "4               chicken breast  "
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Importar la librería pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Especifica la ruta completa del archivo\n",
        "ruta_archivo = r'C:\\Users\\jdvil\\Documents\\Python\\NAACL\\Task_9\\starting_k\\incidents_labelled.csv'\n",
        "\n",
        "# Leer el archivo CSV\n",
        "carga = pd.read_csv(ruta_archivo, encoding='ISO-8859-15')\n",
        "\n",
        "# Mostrar las primeras filas del DataFrame\n",
        "carga.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zejozblZ4X40"
      },
      "outputs": [],
      "source": [
        "df = carga"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4122s2ydwCoz",
        "outputId": "9500dbbc-a67f-4c74-f24e-f8696150abfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Datos faltantes por columna:\n",
            "Unnamed: 0          0\n",
            "year                0\n",
            "month               0\n",
            "day                 0\n",
            "country             0\n",
            "title               0\n",
            "text                0\n",
            "hazard-category     0\n",
            "product-category    0\n",
            "hazard              0\n",
            "product             0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Revisar datos faltantes en cada columna\n",
        "missing_data = df.isnull().sum()\n",
        "print(\"Datos faltantes por columna:\")\n",
        "print(missing_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lWRJmOhw6pa",
        "outputId": "e3d80fa2-32cc-499a-b0ba-4744116e3c08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Caracteres especiales en la columna 'title':\n",
            "Counter({',': 1752, '-': 1581, 'â': 1561, '\\x80': 1559, '.': 1551, 'Ã': 840, '\\x94': 732, ')': 558, '(': 555, '&': 484, \"'\": 390, 'Œ': 339, '\\x99': 319, ':': 258, '\"': 220, '€': 219, 'Â': 189, '\\x93': 186, '\\x9c': 161, '/': 130, 'Î': 122, '®': 111, '¶': 84, '\\x9e': 79, '%': 73, ';': 71, '_': 67, '©': 60, '\\x9d': 60, 'Ï': 56, '\\x9f': 49, '\\x84': 28, '\\x98': 27, '!': 25, '±': 20, '*': 18, '·': 18, '#': 17, '+': 17, 'œ': 17, '\\x96': 17, 'š': 15, '\\x90': 14, '¢': 13, 'º': 13, '\\x83': 12, '¹': 12, '»': 11, '¬': 10, '\\x89': 9, 'µ': 9, '\\x81': 8, 'Ä': 7, '¿': 7, 'Ž': 6, '\\xad': 6, '?': 5, '\\x87': 5, '\\x82': 4, '\\x8d': 4, '³': 4, 'ï': 3, '[': 3, ']': 3, '¡': 3, '\\x95': 3, '\\x86': 3, '²': 3, '\\x92': 3, 'Å': 3, '\\x88': 2, '$': 2, '`': 2, '«': 2, '\\x8e': 1, '\\x9a': 1, '=': 1, 'Š': 1, '¥': 1, '\\x91': 1, '¯': 1, 'ª': 1})\n",
            "\n",
            "Caracteres especiales en la columna 'text':\n",
            "Counter({'.': 110016, ',': 85700, '-': 53594, '/': 46697, '>': 42189, '<': 42171, ':': 35064, ')': 20102, '(': 20090, '\\x80': 18190, 'â': 18142, '\"': 13714, 'Â': 11858, \"'\": 8989, 'Ã': 6379, '&': 4833, '\\x9c': 4750, '\\x9d': 4652, '=': 4545, '\\x99': 4528, ';': 3614, '\\x93': 2604, 'Î': 2270, '?': 2131, 'Œ': 2095, '@': 2081, '#': 1870, '€': 1540, 'Ï': 1110, '[': 749, ']': 743, '\\x97': 702, '\\x8b': 683, '%': 659, '©': 588, '¶': 565, '\\x98': 537, '®': 503, '±': 419, '*': 358, '_': 333, '°': 325, '\\x84': 315, '\\x9f': 306, '¢': 299, '¿': 240, 'µ': 237, 'œ': 232, 'º': 225, '!': 212, '¹': 204, '\\x83': 185, '+': 180, '·': 173, '\\x89': 152, '\\x81': 147, '\\x94': 137, 'š': 132, '\\x82': 132, '\\x90': 117, '\\x9e': 113, '¯': 111, '»': 106, '\\xad': 105, '³': 96, '\\x92': 81, '$': 79, '\\x96': 69, 'Š': 69, '¬': 66, '\\x8c': 62, 'Ž': 56, 'ž': 55, 'Å': 45, '\\x87': 44, '\\x86': 40, '|': 39, '¡': 29, '\\x95': 28, '²': 27, '£': 26, '\\x9a': 25, 'å': 25, '\\x91': 24, '¥': 24, 'ï': 21, '\\x8a': 21, '\\x8d': 21, 'ª': 20, 'Ä': 19, '\\x8e': 19, '\\x88': 18, 'æ': 18, 'Ÿ': 17, '~': 16, '§': 14, 'ç': 13, 'é': 12, '«': 9, '`': 9, 'ã': 8, '\\x9b': 8, 'è': 8, '\\\\': 6, 'Ë': 6, '\\x8f': 6, 'ä': 5, '}': 4, 'ì': 4, 'ë': 3, '{': 2, 'á': 2, 'ê': 1, 'Æ': 1, 'Ì': 1})\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# Definir función para extraer caracteres especiales\n",
        "def extract_special_characters(column):\n",
        "    special_characters = re.findall(r'[^a-zA-Z0-9\\s]', ''.join(column))\n",
        "    return Counter(special_characters)\n",
        "\n",
        "# Obtener caracteres especiales y su concurrencia en las columnas 'title' y 'text'\n",
        "special_characters_title = extract_special_characters(df['title'].astype(str))\n",
        "special_characters_text = extract_special_characters(df['text'].astype(str))\n",
        "\n",
        "print(\"Caracteres especiales en la columna 'title':\")\n",
        "print(special_characters_title)\n",
        "\n",
        "print(\"\\nCaracteres especiales en la columna 'text':\")\n",
        "print(special_characters_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "_biNrsV72R24",
        "outputId": "c29257d2-8221-4c56-f9b1-0d987c466625"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title_clean</th>\n",
              "      <th>text_clean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Recall Notification: FSIS-024-94</td>\n",
              "      <td>Case Number: 024-94   \\n            Date Opene...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Recall Notification: FSIS-033-94</td>\n",
              "      <td>Case Number: 033-94   \\n            Date Opene...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Recall Notification: FSIS-014-94</td>\n",
              "      <td>Case Number: 014-94   \\n            Date Opene...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Recall Notification: FSIS-009-94</td>\n",
              "      <td>Case Number: 009-94   \\n            Date Opene...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Recall Notification: FSIS-001-94</td>\n",
              "      <td>Case Number: 001-94   \\n            Date Opene...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        title_clean  \\\n",
              "0  Recall Notification: FSIS-024-94   \n",
              "1  Recall Notification: FSIS-033-94   \n",
              "2  Recall Notification: FSIS-014-94   \n",
              "3  Recall Notification: FSIS-009-94   \n",
              "4  Recall Notification: FSIS-001-94   \n",
              "\n",
              "                                          text_clean  \n",
              "0  Case Number: 024-94   \\n            Date Opene...  \n",
              "1  Case Number: 033-94   \\n            Date Opene...  \n",
              "2  Case Number: 014-94   \\n            Date Opene...  \n",
              "3  Case Number: 009-94   \\n            Date Opene...  \n",
              "4  Case Number: 001-94   \\n            Date Opene...  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# Función para limpiar texto ignorando errores de codificación\n",
        "def clean_text(text):\n",
        "    # Intentar decodificar correctamente con UTF-8, ignorando errores\n",
        "    try:\n",
        "        # Encode the text to bytes, then decode using utf-8\n",
        "        text = text.encode('latin1', errors='ignore').decode('utf-8', errors='ignore')\n",
        "    except (UnicodeDecodeError, AttributeError):\n",
        "        pass\n",
        "    # Remover cualquier otro carácter especial no ASCII\n",
        "    return re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
        "\n",
        "# Aplicar la limpieza a las columnas 'title' y 'text'\n",
        "df['title_clean'] = df['title'].apply(clean_text)\n",
        "df['text_clean'] = df['text'].apply(clean_text)\n",
        "\n",
        "# Mostrar las primeras filas de las columnas limpiadas\n",
        "df[['title_clean', 'text_clean']].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aq3PuIL22wq6",
        "outputId": "9e0fc60e-9c61-4608-fa67-7850be4af763"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Caracteres especiales en la columna 'title_clean':\n",
            "Counter({',': 1752, '-': 1581, '.': 1551, ')': 558, '(': 555, '&': 484, \"'\": 390, ':': 258, '\"': 220, '/': 130, '%': 73, ';': 71, '_': 67, '!': 25, '*': 18, '#': 17, '+': 17, '?': 5, '[': 3, ']': 3, '$': 2, '`': 2, '=': 1})\n",
            "\n",
            "Caracteres especiales en la columna 'text_clean':\n",
            "Counter({'.': 110016, ',': 85700, '-': 53594, '/': 46697, '>': 42189, '<': 42171, ':': 35064, ')': 20102, '(': 20090, '\"': 13714, \"'\": 8989, '&': 4833, '=': 4545, ';': 3614, '?': 2131, '@': 2081, '#': 1870, '[': 749, ']': 743, '%': 659, '*': 358, '_': 333, '!': 212, '+': 180, '$': 79, '|': 39, '~': 16, '`': 9, '\\\\': 6, '}': 4, '{': 2})\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# Función para extraer caracteres especiales\n",
        "def extract_special_characters(column):\n",
        "    special_characters = re.findall(r'[^a-zA-Z0-9\\s]', ''.join(column))\n",
        "    return Counter(special_characters)\n",
        "\n",
        "# Revisar nuevamente los caracteres especiales en las columnas limpiadas 'title_clean' y 'text_clean'\n",
        "special_characters_title_clean = extract_special_characters(df['title_clean'].astype(str))\n",
        "special_characters_text_clean = extract_special_characters(df['text_clean'].astype(str))\n",
        "\n",
        "# Mostrar los resultados\n",
        "print(\"Caracteres especiales en la columna 'title_clean':\")\n",
        "print(special_characters_title_clean)\n",
        "\n",
        "print(\"\\nCaracteres especiales en la columna 'text_clean':\")\n",
        "print(special_characters_text_clean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "dwYr3e333Lhp",
        "outputId": "2014f6ee-4326-4bc9-b974-b76b474f5c78"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title_clean</th>\n",
              "      <th>text_clean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Recall Notification: FSIS-024-94</td>\n",
              "      <td>Case Number: 024-94                Date Opened...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Recall Notification: FSIS-033-94</td>\n",
              "      <td>Case Number: 033-94                Date Opened...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Recall Notification: FSIS-014-94</td>\n",
              "      <td>Case Number: 014-94                Date Opened...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Recall Notification: FSIS-009-94</td>\n",
              "      <td>Case Number: 009-94                Date Opened...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Recall Notification: FSIS-001-94</td>\n",
              "      <td>Case Number: 001-94                Date Opened...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        title_clean  \\\n",
              "0  Recall Notification: FSIS-024-94   \n",
              "1  Recall Notification: FSIS-033-94   \n",
              "2  Recall Notification: FSIS-014-94   \n",
              "3  Recall Notification: FSIS-009-94   \n",
              "4  Recall Notification: FSIS-001-94   \n",
              "\n",
              "                                          text_clean  \n",
              "0  Case Number: 024-94                Date Opened...  \n",
              "1  Case Number: 033-94                Date Opened...  \n",
              "2  Case Number: 014-94                Date Opened...  \n",
              "3  Case Number: 009-94                Date Opened...  \n",
              "4  Case Number: 001-94                Date Opened...  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Eliminar los saltos de línea (\\n) en las columnas 'title_clean' y 'text_clean'\n",
        "df['title_clean'] = df['title_clean'].str.replace('\\n', ' ')\n",
        "df['text_clean'] = df['text_clean'].str.replace('\\n', ' ')\n",
        "\n",
        "# Mostrar las primeras filas para verificar\n",
        "df[['title_clean', 'text_clean']].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "tDx93TAk24s-",
        "outputId": "d4d2c591-07ef-4083-9656-8af736311f6f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>day</th>\n",
              "      <th>country</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>hazard-category</th>\n",
              "      <th>product-category</th>\n",
              "      <th>hazard</th>\n",
              "      <th>product</th>\n",
              "      <th>title_clean</th>\n",
              "      <th>text_clean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1994</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>us</td>\n",
              "      <td>Recall Notification: FSIS-024-94</td>\n",
              "      <td>Case Number: 024-94   \\n            Date Opene...</td>\n",
              "      <td>biological</td>\n",
              "      <td>meat, egg and dairy products</td>\n",
              "      <td>listeria monocytogenes</td>\n",
              "      <td>smoked sausage</td>\n",
              "      <td>Recall Notification: FSIS-024-94</td>\n",
              "      <td>Case Number: 024-94                Date Opened...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1994</td>\n",
              "      <td>3</td>\n",
              "      <td>10</td>\n",
              "      <td>us</td>\n",
              "      <td>Recall Notification: FSIS-033-94</td>\n",
              "      <td>Case Number: 033-94   \\n            Date Opene...</td>\n",
              "      <td>biological</td>\n",
              "      <td>meat, egg and dairy products</td>\n",
              "      <td>listeria spp</td>\n",
              "      <td>sausage</td>\n",
              "      <td>Recall Notification: FSIS-033-94</td>\n",
              "      <td>Case Number: 033-94                Date Opened...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1994</td>\n",
              "      <td>3</td>\n",
              "      <td>28</td>\n",
              "      <td>us</td>\n",
              "      <td>Recall Notification: FSIS-014-94</td>\n",
              "      <td>Case Number: 014-94   \\n            Date Opene...</td>\n",
              "      <td>biological</td>\n",
              "      <td>meat, egg and dairy products</td>\n",
              "      <td>listeria monocytogenes</td>\n",
              "      <td>ham slices</td>\n",
              "      <td>Recall Notification: FSIS-014-94</td>\n",
              "      <td>Case Number: 014-94                Date Opened...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1994</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>us</td>\n",
              "      <td>Recall Notification: FSIS-009-94</td>\n",
              "      <td>Case Number: 009-94   \\n            Date Opene...</td>\n",
              "      <td>foreign bodies</td>\n",
              "      <td>meat, egg and dairy products</td>\n",
              "      <td>plastic fragment</td>\n",
              "      <td>thermal processed pork meat</td>\n",
              "      <td>Recall Notification: FSIS-009-94</td>\n",
              "      <td>Case Number: 009-94                Date Opened...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>1994</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>us</td>\n",
              "      <td>Recall Notification: FSIS-001-94</td>\n",
              "      <td>Case Number: 001-94   \\n            Date Opene...</td>\n",
              "      <td>foreign bodies</td>\n",
              "      <td>meat, egg and dairy products</td>\n",
              "      <td>plastic fragment</td>\n",
              "      <td>chicken breast</td>\n",
              "      <td>Recall Notification: FSIS-001-94</td>\n",
              "      <td>Case Number: 001-94                Date Opened...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  year  month  day country                             title  \\\n",
              "0           0  1994      1    7      us  Recall Notification: FSIS-024-94   \n",
              "1           1  1994      3   10      us  Recall Notification: FSIS-033-94   \n",
              "2           2  1994      3   28      us  Recall Notification: FSIS-014-94   \n",
              "3           3  1994      4    3      us  Recall Notification: FSIS-009-94   \n",
              "4           4  1994      7    1      us  Recall Notification: FSIS-001-94   \n",
              "\n",
              "                                                text hazard-category  \\\n",
              "0  Case Number: 024-94   \\n            Date Opene...      biological   \n",
              "1  Case Number: 033-94   \\n            Date Opene...      biological   \n",
              "2  Case Number: 014-94   \\n            Date Opene...      biological   \n",
              "3  Case Number: 009-94   \\n            Date Opene...  foreign bodies   \n",
              "4  Case Number: 001-94   \\n            Date Opene...  foreign bodies   \n",
              "\n",
              "               product-category                  hazard  \\\n",
              "0  meat, egg and dairy products  listeria monocytogenes   \n",
              "1  meat, egg and dairy products            listeria spp   \n",
              "2  meat, egg and dairy products  listeria monocytogenes   \n",
              "3  meat, egg and dairy products        plastic fragment   \n",
              "4  meat, egg and dairy products        plastic fragment   \n",
              "\n",
              "                       product                       title_clean  \\\n",
              "0               smoked sausage  Recall Notification: FSIS-024-94   \n",
              "1                      sausage  Recall Notification: FSIS-033-94   \n",
              "2                   ham slices  Recall Notification: FSIS-014-94   \n",
              "3  thermal processed pork meat  Recall Notification: FSIS-009-94   \n",
              "4               chicken breast  Recall Notification: FSIS-001-94   \n",
              "\n",
              "                                          text_clean  \n",
              "0  Case Number: 024-94                Date Opened...  \n",
              "1  Case Number: 033-94                Date Opened...  \n",
              "2  Case Number: 014-94                Date Opened...  \n",
              "3  Case Number: 009-94                Date Opened...  \n",
              "4  Case Number: 001-94                Date Opened...  "
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HU1BEl2-37zU",
        "outputId": "ff3e1165-d105-486e-fbfd-9464e69fd69b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Distribución de 'hazard-category':\n",
            "hazard-category\n",
            "biological                        2020\n",
            "allergens                         1955\n",
            "foreign bodies                     769\n",
            "chemical                           498\n",
            "fraud                              411\n",
            "other hazard                       147\n",
            "packaging defect                    82\n",
            "organoleptic aspects                63\n",
            "food additives and flavourings      26\n",
            "migration                           13\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Distribución de 'product-category':\n",
            "product-category\n",
            "meat, egg and dairy products                         1687\n",
            "cereals and bakery products                           778\n",
            "fruits and vegetables                                 655\n",
            "prepared dishes and snacks                            516\n",
            "nuts, nut products and seeds                          321\n",
            "soups, broths, sauces and condiments                  302\n",
            "seafood                                               297\n",
            "cocoa and cocoa preparations, coffee and tea          245\n",
            "ices and desserts                                     232\n",
            "confectionery                                         194\n",
            "herbs and spices                                      177\n",
            "dietetic foods, food supplements, fortified foods     172\n",
            "non-alcoholic beverages                               170\n",
            "alcoholic beverages                                    76\n",
            "other food product / mixed                             63\n",
            "food contact materials                                 25\n",
            "fats and oils                                          22\n",
            "pet feed                                               20\n",
            "food additives and flavourings                         11\n",
            "honey and royal jelly                                  10\n",
            "feed materials                                          6\n",
            "sugars and syrups                                       5\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Revisar la distribución de las columnas objetivo: hazard-category y product-category\n",
        "hazard_distribution = df['hazard-category'].value_counts()\n",
        "product_distribution = df['product-category'].value_counts()\n",
        "\n",
        "# Mostrar la distribución\n",
        "print(\"Distribución de 'hazard-category':\")\n",
        "print(hazard_distribution)\n",
        "\n",
        "print(\"\\nDistribución de 'product-category':\")\n",
        "print(product_distribution)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fpbuqG4drlp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_sV4tIfj9px"
      },
      "source": [
        "# Modelo Base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaC-_nu9hVwT",
        "outputId": "083513b6-5da3-405b-acad-f1f5d9bd9257"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluación del modelo para 'hazard-category':\n",
            "                                precision    recall  f1-score   support\n",
            "\n",
            "                     allergens       0.87      0.83      0.85       391\n",
            "                    biological       0.88      0.77      0.82       404\n",
            "                      chemical       0.34      0.74      0.47       100\n",
            "food additives and flavourings       0.00      0.00      0.00         5\n",
            "                foreign bodies       0.64      0.53      0.58       154\n",
            "                         fraud       0.37      0.56      0.45        82\n",
            "                     migration       0.00      0.00      0.00         3\n",
            "          organoleptic aspects       0.00      0.00      0.00        13\n",
            "                  other hazard       1.00      0.03      0.07        29\n",
            "              packaging defect       0.00      0.00      0.00        16\n",
            "\n",
            "                      accuracy                           0.70      1197\n",
            "                     macro avg       0.41      0.35      0.32      1197\n",
            "                  weighted avg       0.74      0.70      0.70      1197\n",
            "\n",
            "Matriz de confusión:\n",
            "[[326   3  23   0   0  39   0   0   0   0]\n",
            " [  8 311  70   0  12   2   0   0   0   1]\n",
            " [  5   9  74   0   9   3   0   0   0   0]\n",
            " [  1   0   0   0   3   1   0   0   0   0]\n",
            " [ 10  12  29   0  81  22   0   0   0   0]\n",
            " [ 18   4  10   0   4  46   0   0   0   0]\n",
            " [  0   0   3   0   0   0   0   0   0   0]\n",
            " [  5   3   1   0   4   0   0   0   0   0]\n",
            " [  0   6   5   0   6  10   0   0   1   1]\n",
            " [  2   4   3   0   7   0   0   0   0   0]]\n",
            "Accuracy: 0.7009189640768588\n",
            "F1 Score: 0.7013994550591638\n",
            "Precision: 0.7430778078083728\n",
            "Recall: 0.7009189640768588\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluación del modelo para 'product-category':\n",
            "                                                   precision    recall  f1-score   support\n",
            "\n",
            "                              alcoholic beverages       0.00      0.00      0.00        16\n",
            "                      cereals and bakery products       0.21      0.74      0.33       141\n",
            "     cocoa and cocoa preparations, coffee and tea       0.71      0.22      0.33        55\n",
            "                                    confectionery       0.00      0.00      0.00        34\n",
            "dietetic foods, food supplements, fortified foods       1.00      0.18      0.31        38\n",
            "                                    fats and oils       1.00      0.12      0.22         8\n",
            "                                   feed materials       0.00      0.00      0.00         1\n",
            "                   food additives and flavourings       0.00      0.00      0.00         1\n",
            "                           food contact materials       0.00      0.00      0.00         7\n",
            "                            fruits and vegetables       0.39      0.53      0.45       130\n",
            "                                 herbs and spices       0.13      0.06      0.08        35\n",
            "                            honey and royal jelly       0.00      0.00      0.00         2\n",
            "                                ices and desserts       0.90      0.54      0.68        48\n",
            "                     meat, egg and dairy products       0.68      0.56      0.62       319\n",
            "                          non-alcoholic beverages       0.56      0.14      0.23        35\n",
            "                     nuts, nut products and seeds       0.38      0.32      0.35        63\n",
            "                       other food product / mixed       0.00      0.00      0.00        13\n",
            "                                         pet feed       0.00      0.00      0.00         2\n",
            "                       prepared dishes and snacks       0.29      0.21      0.24       110\n",
            "                                          seafood       0.44      0.31      0.36        65\n",
            "             soups, broths, sauces and condiments       0.56      0.07      0.12        72\n",
            "                                sugars and syrups       0.00      0.00      0.00         2\n",
            "\n",
            "                                         accuracy                           0.40      1197\n",
            "                                        macro avg       0.33      0.18      0.20      1197\n",
            "                                     weighted avg       0.48      0.40      0.38      1197\n",
            "\n",
            "Matriz de confusión:\n",
            "[[  0  13   0   0   0   0   0   0   0   3   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0]\n",
            " [  0 105   0   0   0   0   0   0   0   8   3   0   1   6   2   5   0   0\n",
            "    5   6   0   0]\n",
            " [  0  26  12   1   0   0   0   0   0   1   0   0   0   7   0   6   0   0\n",
            "    1   1   0   0]\n",
            " [  0  22   0   0   0   0   0   0   0   4   0   0   0   0   0   1   0   0\n",
            "    5   2   0   0]\n",
            " [  0  25   1   0   7   0   0   0   0   3   0   0   0   1   0   0   0   0\n",
            "    1   0   0   0]\n",
            " [  0   1   0   0   0   1   0   0   0   3   1   0   1   1   0   0   0   0\n",
            "    0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0\n",
            "    0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0\n",
            "    0   0   0   0]\n",
            " [  0   6   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0\n",
            "    0   0   0   0]\n",
            " [  0  42   0   0   0   0   0   0   0  69   2   0   0   5   0   3   1   0\n",
            "    5   2   1   0]\n",
            " [  0  18   0   0   0   0   0   0   0   8   2   0   0   1   0   2   0   0\n",
            "    2   1   1   0]\n",
            " [  0   1   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0]\n",
            " [  0  11   1   0   0   0   0   0   0   2   0   0  26   3   0   4   0   0\n",
            "    1   0   0   0]\n",
            " [  0  84   2   0   0   0   0   0   0  36   1   0   0 180   2   3   1   0\n",
            "    7   1   2   0]\n",
            " [  0  21   0   0   0   0   0   0   0   2   0   0   1   1   5   1   0   0\n",
            "    2   2   0   0]\n",
            " [  0  28   1   0   0   0   0   0   0   8   1   0   0   4   0  20   0   0\n",
            "    1   0   0   0]\n",
            " [  0   3   0   0   0   0   0   0   0   4   1   0   0   3   0   1   0   0\n",
            "    0   1   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   1   0   0   0   1   0   0   0   0\n",
            "    0   0   0   0]\n",
            " [  0  43   0   0   0   0   0   0   0  11   2   0   0  21   0   5   0   0\n",
            "   23   5   0   0]\n",
            " [  0  12   0   0   0   0   0   0   0   8   1   0   0  19   0   1   0   0\n",
            "    4  20   0   0]\n",
            " [  0  28   0   0   0   0   0   0   0   6   0   0   0   9   0   0   0   0\n",
            "   20   4   5   0]\n",
            " [  0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0\n",
            "    1   0   0   0]]\n",
            "Accuracy: 0.3968253968253968\n",
            "F1 Score: 0.38303762516846623\n",
            "Precision: 0.4807121840436586\n",
            "Recall: 0.3968253968253968\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "# Importar las librerías necesarias\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Paso 1: Combinar las columnas 'title_clean' y 'text_clean' en una sola\n",
        "df['combined_text'] = df['text_clean']\n",
        "\n",
        "# Paso 2: Codificar las etiquetas\n",
        "# Codificar 'hazard-category'\n",
        "le_hazard = LabelEncoder()\n",
        "df['hazard_encoded'] = le_hazard.fit_transform(df['hazard-category'])\n",
        "\n",
        "# Codificar 'product-category' dentro de cada 'hazard-category'\n",
        "# Crear un diccionario para almacenar los codificadores de 'product-category' por 'hazard-category'\n",
        "product_encoders = {}\n",
        "df['product_encoded'] = -1  # Inicializar con -1\n",
        "\n",
        "# Obtener las categorías únicas de 'hazard-category'\n",
        "hazard_categories = df['hazard-category'].unique()\n",
        "\n",
        "for hazard in hazard_categories:\n",
        "    # Filtrar el DataFrame por 'hazard-category'\n",
        "    mask = df['hazard-category'] == hazard\n",
        "    # Crear un LabelEncoder para el 'hazard-category' actual\n",
        "    le_product = LabelEncoder()\n",
        "    df.loc[mask, 'product_encoded'] = le_product.fit_transform(df.loc[mask, 'product-category'])\n",
        "    # Almacenar el codificador\n",
        "    product_encoders[hazard] = le_product\n",
        "\n",
        "# Paso 3: Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X = df['combined_text']\n",
        "y_hazard = df['hazard_encoded']\n",
        "y_product = df['product_encoded']\n",
        "\n",
        "X_train, X_test, y_hazard_train, y_hazard_test, y_product_train, y_product_test = train_test_split(\n",
        "    X, y_hazard, y_product, test_size=0.2, random_state=42, stratify=y_hazard\n",
        ")\n",
        "\n",
        "# Paso 4: Crear un Pipeline para el modelo de Naive Bayes\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "# Crear un Pipeline para 'hazard-category'\n",
        "pipeline_hazard = Pipeline([\n",
        "    ('vectorizer', CountVectorizer()),\n",
        "    ('classifier', MultinomialNB())\n",
        "])\n",
        "\n",
        "# Entrenar el modelo para 'hazard-category'\n",
        "pipeline_hazard.fit(X_train, y_hazard_train)\n",
        "\n",
        "# Predecir en el conjunto de prueba\n",
        "y_hazard_pred = pipeline_hazard.predict(X_test)\n",
        "\n",
        "# Paso 5: Evaluar el modelo para 'hazard-category'\n",
        "print(\"Evaluación del modelo para 'hazard-category':\")\n",
        "print(classification_report(y_hazard_test, y_hazard_pred, target_names=le_hazard.classes_))\n",
        "print(\"Matriz de confusión:\")\n",
        "print(confusion_matrix(y_hazard_test, y_hazard_pred))\n",
        "print(\"Accuracy:\", accuracy_score(y_hazard_test, y_hazard_pred))\n",
        "print(\"F1 Score:\", f1_score(y_hazard_test, y_hazard_pred, average='weighted'))\n",
        "print(\"Precision:\", precision_score(y_hazard_test, y_hazard_pred, average='weighted'))\n",
        "print(\"Recall:\", recall_score(y_hazard_test, y_hazard_pred, average='weighted'))\n",
        "\n",
        "# Paso 6: Para 'product-category', necesitamos tener en cuenta la jerarquía\n",
        "# Crear un diccionario para almacenar los modelos por 'hazard-category'\n",
        "product_models = {}\n",
        "\n",
        "# Entrenar un modelo para cada 'hazard-category'\n",
        "for hazard in hazard_categories:\n",
        "    # Filtrar los datos correspondientes\n",
        "    mask_train = y_hazard_train == le_hazard.transform([hazard])[0]\n",
        "    mask_test = y_hazard_test == le_hazard.transform([hazard])[0]\n",
        "\n",
        "    X_train_hazard = X_train[mask_train]\n",
        "    y_product_train_hazard = y_product_train[mask_train]\n",
        "\n",
        "    # Solo entrenar si hay suficientes muestras\n",
        "    if len(X_train_hazard) > 0:\n",
        "        # Crear y entrenar el modelo\n",
        "        pipeline_product = Pipeline([\n",
        "            ('vectorizer', CountVectorizer()),\n",
        "            ('classifier', MultinomialNB())\n",
        "        ])\n",
        "        pipeline_product.fit(X_train_hazard, y_product_train_hazard)\n",
        "        # Almacenar el modelo\n",
        "        product_models[hazard] = pipeline_product\n",
        "\n",
        "# Paso 7: Predecir 'product-category' teniendo en cuenta 'hazard-category'\n",
        "y_product_pred = []\n",
        "\n",
        "for i in range(len(X_test)):\n",
        "    hazard_idx = y_hazard_pred[i]\n",
        "    hazard_label = le_hazard.inverse_transform([hazard_idx])[0]\n",
        "\n",
        "    # Si tenemos un modelo para este 'hazard-category'\n",
        "    if hazard_label in product_models:\n",
        "        model = product_models[hazard_label]\n",
        "        # Transformar el texto en una lista para el modelo\n",
        "        X_sample = [X_test.iloc[i]]\n",
        "        # Predecir 'product-category'\n",
        "        product_pred_encoded = model.predict(X_sample)[0]\n",
        "        # Decodificar la predicción\n",
        "        le_product = product_encoders[hazard_label]\n",
        "        product_pred_label = le_product.inverse_transform([product_pred_encoded])[0]\n",
        "        # Codificar de nuevo con números únicos para evaluación\n",
        "        y_product_pred.append((hazard_label, product_pred_label))\n",
        "    else:\n",
        "        # Si no hay modelo, asignar una etiqueta por defecto\n",
        "        y_product_pred.append((hazard_label, None))\n",
        "\n",
        "# Paso 8: Evaluar el modelo para 'product-category'\n",
        "# Necesitamos comparar las predicciones con las etiquetas verdaderas\n",
        "y_product_true_labels = []\n",
        "y_product_pred_labels = []\n",
        "\n",
        "for i in range(len(y_product_test)):\n",
        "    hazard_idx = y_hazard_test.iloc[i]\n",
        "    hazard_label = le_hazard.inverse_transform([hazard_idx])[0]\n",
        "    product_idx = y_product_test.iloc[i]\n",
        "    le_product = product_encoders[hazard_label]\n",
        "    product_label = le_product.inverse_transform([product_idx])[0]\n",
        "\n",
        "    y_product_true_labels.append((hazard_label, product_label))\n",
        "\n",
        "    # Obtener la predicción correspondiente\n",
        "    pred_hazard_label, pred_product_label = y_product_pred[i]\n",
        "    y_product_pred_labels.append((pred_hazard_label, pred_product_label))\n",
        "\n",
        "# Evaluar solo las instancias donde tenemos predicciones válidas\n",
        "true_labels = []\n",
        "pred_labels = []\n",
        "\n",
        "for true, pred in zip(y_product_true_labels, y_product_pred_labels):\n",
        "    # Si la predicción no es None\n",
        "    if pred[1] is not None:\n",
        "        true_labels.append(true[1])\n",
        "        pred_labels.append(pred[1])\n",
        "\n",
        "# Reporte de clasificación para 'product-category'\n",
        "print(\"\\nEvaluación del modelo para 'product-category':\")\n",
        "print(classification_report(true_labels, pred_labels))\n",
        "print(\"Matriz de confusión:\")\n",
        "print(confusion_matrix(true_labels, pred_labels))\n",
        "print(\"Accuracy:\", accuracy_score(true_labels, pred_labels))\n",
        "print(\"F1 Score:\", f1_score(true_labels, pred_labels, average='weighted'))\n",
        "print(\"Precision:\", precision_score(true_labels, pred_labels, average='weighted'))\n",
        "print(\"Recall:\", recall_score(true_labels, pred_labels, average='weighted'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15vUkYRj2LqI"
      },
      "source": [
        "# HiperParamnetros Base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86FSgIU_kBNn",
        "outputId": "23e2ce7c-1208-4b31-9dd2-5d1b04ce8b6e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mejores hiperparámetros para hazard-category 'biological': {'classifier__alpha': 0.1, 'vectorizer__ngram_range': (1, 1)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mejores hiperparámetros para hazard-category 'foreign bodies': {'classifier__alpha': 0.1, 'vectorizer__ngram_range': (1, 1)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mejores hiperparámetros para hazard-category 'chemical': {'classifier__alpha': 0.3, 'vectorizer__ngram_range': (1, 1)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mejores hiperparámetros para hazard-category 'fraud': {'classifier__alpha': 0.1, 'vectorizer__ngram_range': (1, 1)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mejores hiperparámetros para hazard-category 'organoleptic aspects': {'classifier__alpha': 1.0, 'vectorizer__ngram_range': (1, 2)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mejores hiperparámetros para hazard-category 'allergens': {'classifier__alpha': 0.3, 'vectorizer__ngram_range': (1, 1)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mejores hiperparámetros para hazard-category 'packaging defect': {'classifier__alpha': 0.1, 'vectorizer__ngram_range': (1, 1)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mejores hiperparámetros para hazard-category 'other hazard': {'classifier__alpha': 0.1, 'vectorizer__ngram_range': (1, 1)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mejores hiperparámetros para hazard-category 'food additives and flavourings': {'classifier__alpha': 0.1, 'vectorizer__ngram_range': (2, 2)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mejores hiperparámetros para hazard-category 'migration': {'classifier__alpha': 0.1, 'vectorizer__ngram_range': (2, 2)}\n",
            "\n",
            "Evaluación del modelo para 'product-category':\n",
            "                                                   precision    recall  f1-score   support\n",
            "\n",
            "                              alcoholic beverages       0.25      0.06      0.10        16\n",
            "                      cereals and bakery products       0.30      0.57      0.40       141\n",
            "     cocoa and cocoa preparations, coffee and tea       0.32      0.36      0.34        55\n",
            "                                    confectionery       0.33      0.21      0.25        34\n",
            "dietetic foods, food supplements, fortified foods       0.62      0.34      0.44        38\n",
            "                                    fats and oils       0.00      0.00      0.00         8\n",
            "                                   feed materials       0.00      0.00      0.00         1\n",
            "                   food additives and flavourings       0.00      0.00      0.00         1\n",
            "                           food contact materials       1.00      0.43      0.60         7\n",
            "                            fruits and vegetables       0.42      0.51      0.46       130\n",
            "                                 herbs and spices       0.15      0.14      0.15        35\n",
            "                            honey and royal jelly       0.00      0.00      0.00         2\n",
            "                                ices and desserts       0.67      0.71      0.69        48\n",
            "                     meat, egg and dairy products       0.77      0.56      0.65       319\n",
            "                          non-alcoholic beverages       0.36      0.40      0.38        35\n",
            "                     nuts, nut products and seeds       0.47      0.33      0.39        63\n",
            "                       other food product / mixed       0.00      0.00      0.00        13\n",
            "                                         pet feed       0.33      1.00      0.50         2\n",
            "                       prepared dishes and snacks       0.34      0.30      0.32       110\n",
            "                                          seafood       0.35      0.54      0.42        65\n",
            "             soups, broths, sauces and condiments       0.38      0.26      0.31        72\n",
            "                                sugars and syrups       0.00      0.00      0.00         2\n",
            "\n",
            "                                         accuracy                           0.45      1197\n",
            "                                        macro avg       0.32      0.31      0.29      1197\n",
            "                                     weighted avg       0.48      0.45      0.45      1197\n",
            "\n",
            "Matriz de confusión:\n",
            "[[  1   5   1   0   0   0   0   0   0   2   0   0   0   0   4   0   0   0\n",
            "    1   2   0   0]\n",
            " [  0  81  10   2   1   0   0   0   0  12   2   0   4   4   2   3   0   0\n",
            "    8  10   2   0]\n",
            " [  0  14  20   3   2   0   0   0   0   2   3   0   1   2   0   5   0   0\n",
            "    1   2   0   0]\n",
            " [  2   9   6   7   0   0   0   0   0   4   0   0   1   0   0   0   0   0\n",
            "    1   2   2   0]\n",
            " [  0  12   2   1  13   0   0   0   0   1   0   0   0   1   2   0   0   0\n",
            "    3   2   1   0]\n",
            " [  0   1   0   0   0   0   0   0   0   2   1   0   0   1   0   0   0   0\n",
            "    0   2   1   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   1   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0\n",
            "    0   0   0   0]\n",
            " [  1   2   0   0   0   0   0   0   3   0   0   0   0   1   0   0   0   0\n",
            "    0   0   0   0]\n",
            " [  0  22   3   2   1   0   0   0   0  66   6   0   2   6   4   5   0   0\n",
            "    4   5   4   0]\n",
            " [  0  11   0   2   0   0   0   0   0   9   5   0   1   0   0   2   0   0\n",
            "    2   2   1   0]\n",
            " [  0   1   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0]\n",
            " [  0   3   1   1   0   1   0   0   0   0   0   0  34   1   2   1   0   0\n",
            "    1   2   1   0]\n",
            " [  0  54   7   0   0   1   0   0   0  26   5   0   1 179   6   2   0   2\n",
            "   15  13   8   0]\n",
            " [  0   7   1   0   1   1   0   0   0   4   0   0   5   0  14   0   0   0\n",
            "    0   1   1   0]\n",
            " [  0  13   5   2   0   0   0   0   0   7   4   0   0   3   0  21   0   2\n",
            "    3   2   1   0]\n",
            " [  0   1   0   0   2   0   0   0   0   2   0   0   0   3   0   1   0   0\n",
            "    2   2   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   2\n",
            "    0   0   0   0]\n",
            " [  0  21   3   1   0   0   0   0   0  12   5   0   1  18   0   4   0   0\n",
            "   33   9   3   0]\n",
            " [  0   3   1   0   0   0   0   0   0   2   1   0   0  10   1   1   0   0\n",
            "    6  35   5   0]\n",
            " [  0   9   3   0   1   0   0   0   0   7   0   0   1   4   4   0   0   0\n",
            "   16   8  19   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    1   1   0   0]]\n",
            "Accuracy: 0.4452798663324979\n",
            "F1 Score: 0.4459113322996568\n",
            "Precision: 0.47808865748797275\n",
            "Recall: 0.4452798663324979\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "# Importar las librerías necesarias\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Paso 1: Usar 'text_clean' como entrada\n",
        "df['combined_text'] = df['text_clean']\n",
        "\n",
        "# Paso 2: Codificar las etiquetas\n",
        "le_hazard = LabelEncoder()\n",
        "df['hazard_encoded'] = le_hazard.fit_transform(df['hazard-category'])\n",
        "\n",
        "product_encoders = {}\n",
        "df['product_encoded'] = -1  # Inicializar con -1\n",
        "\n",
        "# Codificar 'product-category' dentro de cada 'hazard-category'\n",
        "hazard_categories = df['hazard-category'].unique()\n",
        "for hazard in hazard_categories:\n",
        "    mask = df['hazard-category'] == hazard\n",
        "    le_product = LabelEncoder()\n",
        "    df.loc[mask, 'product_encoded'] = le_product.fit_transform(df.loc[mask, 'product-category'])\n",
        "    product_encoders[hazard] = le_product\n",
        "\n",
        "# Paso 3: Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X = df['combined_text']\n",
        "y_hazard = df['hazard_encoded']\n",
        "y_product = df['product_encoded']\n",
        "\n",
        "X_train, X_test, y_hazard_train, y_hazard_test, y_product_train, y_product_test = train_test_split(\n",
        "    X, y_hazard, y_product, test_size=0.2, random_state=42, stratify=y_hazard\n",
        ")\n",
        "\n",
        "# Paso 4: Crear un Pipeline y buscar hiperparámetros para 'product-category'\n",
        "# Definir el pipeline para el producto\n",
        "pipeline_product = Pipeline([\n",
        "    ('vectorizer', CountVectorizer()),\n",
        "    ('classifier', MultinomialNB())\n",
        "])\n",
        "\n",
        "# Definir los parámetros que se van a probar en la búsqueda\n",
        "param_grid = {\n",
        "    'vectorizer__ngram_range': [(1, 1), (1, 2), (2, 2)],  # Unigramas y bigramas\n",
        "    'classifier__alpha': [0.1,0.3, 0.5, 0.7,1.0]  # Parámetros de suavizado para Naive Bayes\n",
        "}\n",
        "\n",
        "# Crear el modelo de búsqueda\n",
        "grid_search = GridSearchCV(pipeline_product, param_grid, scoring='f1_weighted', cv=5)\n",
        "\n",
        "# Entrenar un modelo para cada 'hazard-category'\n",
        "product_models = {}\n",
        "\n",
        "for hazard in hazard_categories:\n",
        "    # Filtrar los datos correspondientes\n",
        "    mask_train = y_hazard_train == le_hazard.transform([hazard])[0]\n",
        "\n",
        "    X_train_hazard = X_train[mask_train]\n",
        "    y_product_train_hazard = y_product_train[mask_train]\n",
        "\n",
        "    if len(X_train_hazard) > 0:\n",
        "        # Realizar la búsqueda de hiperparámetros\n",
        "        grid_search.fit(X_train_hazard, y_product_train_hazard)\n",
        "        # Almacenar el mejor modelo\n",
        "        product_models[hazard] = grid_search.best_estimator_\n",
        "\n",
        "        # Mostrar los mejores hiperparámetros encontrados\n",
        "        print(f\"Mejores hiperparámetros para hazard-category '{hazard}': {grid_search.best_params_}\")\n",
        "\n",
        "# Paso 5: Predecir 'product-category' teniendo en cuenta 'hazard-category'\n",
        "y_product_pred = []\n",
        "\n",
        "for i in range(len(X_test)):\n",
        "    hazard_idx = y_hazard_pred[i]\n",
        "    hazard_label = le_hazard.inverse_transform([hazard_idx])[0]\n",
        "\n",
        "    if hazard_label in product_models:\n",
        "        model = product_models[hazard_label]\n",
        "        X_sample = [X_test.iloc[i]]\n",
        "        product_pred_encoded = model.predict(X_sample)[0]\n",
        "        le_product = product_encoders[hazard_label]\n",
        "        product_pred_label = le_product.inverse_transform([product_pred_encoded])[0]\n",
        "        y_product_pred.append((hazard_label, product_pred_label))\n",
        "\n",
        "# Paso 6: Evaluar el modelo para 'product-category'\n",
        "y_product_true_labels = []\n",
        "y_product_pred_labels = []\n",
        "\n",
        "for i in range(len(y_product_test)):\n",
        "    hazard_idx = y_hazard_test.iloc[i]\n",
        "    hazard_label = le_hazard.inverse_transform([hazard_idx])[0]\n",
        "    product_idx = y_product_test.iloc[i]\n",
        "    le_product = product_encoders[hazard_label]\n",
        "    product_label = le_product.inverse_transform([product_idx])[0]\n",
        "\n",
        "    y_product_true_labels.append((hazard_label, product_label))\n",
        "\n",
        "    pred_hazard_label, pred_product_label = y_product_pred[i]\n",
        "    y_product_pred_labels.append((pred_hazard_label, pred_product_label))\n",
        "\n",
        "# Evaluar solo las instancias donde tenemos predicciones válidas\n",
        "true_labels = []\n",
        "pred_labels = []\n",
        "\n",
        "for true, pred in zip(y_product_true_labels, y_product_pred_labels):\n",
        "    true_labels.append(true[1])\n",
        "    pred_labels.append(pred[1])\n",
        "\n",
        "# Reporte de clasificación para 'product-category'\n",
        "print(\"\\nEvaluación del modelo para 'product-category':\")\n",
        "print(classification_report(true_labels, pred_labels))\n",
        "print(\"Matriz de confusión:\")\n",
        "print(confusion_matrix(true_labels, pred_labels))\n",
        "print(\"Accuracy:\", accuracy_score(true_labels, pred_labels))\n",
        "print(\"F1 Score:\", f1_score(true_labels, pred_labels, average='weighted'))\n",
        "print(\"Precision:\", precision_score(true_labels, pred_labels, average='weighted'))\n",
        "print(\"Recall:\", recall_score(true_labels, pred_labels, average='weighted'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "id2AWiCu2RaF"
      },
      "source": [
        "# HiperParamnetros Base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8TGYG_DkBJF",
        "outputId": "76377edb-cd92-4435-b984-87cd79e4bde4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---- Validación cruzada: Fold 1 ----\n",
            "Mejores hiperparámetros para predecir 'product-category' en hazard-category 'biological' en Fold 1: {'classifier__alpha': 0.5, 'vectorizer__ngram_range': (1, 1)}\n",
            "Mejores hiperparámetros para predecir 'product-category' en hazard-category 'foreign bodies' en Fold 1: {'classifier__alpha': 0.1, 'vectorizer__ngram_range': (1, 1)}\n",
            "Mejores hiperparámetros para predecir 'product-category' en hazard-category 'chemical' en Fold 1: {'classifier__alpha': 0.3, 'vectorizer__ngram_range': (1, 3)}\n",
            "Mejores hiperparámetros para predecir 'product-category' en hazard-category 'fraud' en Fold 1: {'classifier__alpha': 0.1, 'vectorizer__ngram_range': (1, 1)}\n",
            "Mejores hiperparámetros para predecir 'product-category' en hazard-category 'organoleptic aspects' en Fold 1: {'classifier__alpha': 0.7, 'vectorizer__ngram_range': (2, 2)}\n",
            "Mejores hiperparámetros para predecir 'product-category' en hazard-category 'allergens' en Fold 1: {'classifier__alpha': 0.3, 'vectorizer__ngram_range': (1, 1)}\n",
            "Mejores hiperparámetros para predecir 'product-category' en hazard-category 'packaging defect' en Fold 1: {'classifier__alpha': 1.0, 'vectorizer__ngram_range': (2, 2)}\n",
            "Mejores hiperparámetros para predecir 'product-category' en hazard-category 'other hazard' en Fold 1: {'classifier__alpha': 0.1, 'vectorizer__ngram_range': (1, 1)}\n",
            "Mejores hiperparámetros para predecir 'product-category' en hazard-category 'food additives and flavourings' en Fold 1: {'classifier__alpha': 0.1, 'vectorizer__ngram_range': (1, 1)}\n",
            "Mejores hiperparámetros para predecir 'product-category' en hazard-category 'migration' en Fold 1: {'classifier__alpha': 1.0, 'vectorizer__ngram_range': (1, 2)}\n",
            "F1-Score de 'product-category' en Fold 1: 0.4700907849805738\n",
            "---- Validación cruzada: Fold 2 ----\n",
            "Mejores hiperparámetros para predecir 'product-category' en hazard-category 'biological' en Fold 2: {'classifier__alpha': 0.1, 'vectorizer__ngram_range': (1, 1)}\n",
            "Mejores hiperparámetros para predecir 'product-category' en hazard-category 'foreign bodies' en Fold 2: {'classifier__alpha': 0.5, 'vectorizer__ngram_range': (1, 1)}\n",
            "Mejores hiperparámetros para predecir 'product-category' en hazard-category 'chemical' en Fold 2: {'classifier__alpha': 0.5, 'vectorizer__ngram_range': (1, 1)}\n",
            "Mejores hiperparámetros para predecir 'product-category' en hazard-category 'fraud' en Fold 2: {'classifier__alpha': 0.1, 'vectorizer__ngram_range': (1, 1)}\n",
            "Mejores hiperparámetros para predecir 'product-category' en hazard-category 'organoleptic aspects' en Fold 2: {'classifier__alpha': 1.0, 'vectorizer__ngram_range': (2, 2)}\n",
            "Mejores hiperparámetros para predecir 'product-category' en hazard-category 'allergens' en Fold 2: {'classifier__alpha': 0.3, 'vectorizer__ngram_range': (1, 1)}\n",
            "Mejores hiperparámetros para predecir 'product-category' en hazard-category 'packaging defect' en Fold 2: {'classifier__alpha': 1.0, 'vectorizer__ngram_range': (2, 2)}\n",
            "Mejores hiperparámetros para predecir 'product-category' en hazard-category 'other hazard' en Fold 2: {'classifier__alpha': 0.1, 'vectorizer__ngram_range': (1, 1)}\n",
            "Mejores hiperparámetros para predecir 'product-category' en hazard-category 'food additives and flavourings' en Fold 2: {'classifier__alpha': 0.1, 'vectorizer__ngram_range': (1, 1)}\n",
            "Mejores hiperparámetros para predecir 'product-category' en hazard-category 'migration' en Fold 2: {'classifier__alpha': 0.1, 'vectorizer__ngram_range': (2, 2)}\n",
            "F1-Score de 'product-category' en Fold 2: 0.4561835721839729\n",
            "---- Validación cruzada: Fold 3 ----\n",
            "Mejores hiperparámetros para predecir 'product-category' en hazard-category 'biological' en Fold 3: {'classifier__alpha': 0.1, 'vectorizer__ngram_range': (1, 1)}\n",
            "Mejores hiperparámetros para predecir 'product-category' en hazard-category 'foreign bodies' en Fold 3: {'classifier__alpha': 0.1, 'vectorizer__ngram_range': (1, 1)}\n",
            "Mejores hiperparámetros para predecir 'product-category' en hazard-category 'chemical' en Fold 3: {'classifier__alpha': 0.3, 'vectorizer__ngram_range': (1, 3)}\n",
            "Mejores hiperparámetros para predecir 'product-category' en hazard-category 'fraud' en Fold 3: {'classifier__alpha': 0.3, 'vectorizer__ngram_range': (1, 1)}\n",
            "Mejores hiperparámetros para predecir 'product-category' en hazard-category 'organoleptic aspects' en Fold 3: {'classifier__alpha': 0.7, 'vectorizer__ngram_range': (1, 1)}\n",
            "Mejores hiperparámetros para predecir 'product-category' en hazard-category 'allergens' en Fold 3: {'classifier__alpha': 0.7, 'vectorizer__ngram_range': (1, 1)}\n",
            "Mejores hiperparámetros para predecir 'product-category' en hazard-category 'packaging defect' en Fold 3: {'classifier__alpha': 0.7, 'vectorizer__ngram_range': (1, 1)}\n",
            "Mejores hiperparámetros para predecir 'product-category' en hazard-category 'other hazard' en Fold 3: {'classifier__alpha': 0.1, 'vectorizer__ngram_range': (1, 1)}\n",
            "Mejores hiperparámetros para predecir 'product-category' en hazard-category 'food additives and flavourings' en Fold 3: {'classifier__alpha': 1.0, 'vectorizer__ngram_range': (1, 1)}\n",
            "Mejores hiperparámetros para predecir 'product-category' en hazard-category 'migration' en Fold 3: {'classifier__alpha': 0.5, 'vectorizer__ngram_range': (2, 2)}\n",
            "F1-Score de 'product-category' en Fold 3: 0.46843332627816403\n",
            "---- Validación cruzada: Fold 4 ----\n",
            "Mejores hiperparámetros para predecir 'product-category' en hazard-category 'biological' en Fold 4: {'classifier__alpha': 0.1, 'vectorizer__ngram_range': (1, 1)}\n",
            "Mejores hiperparámetros para predecir 'product-category' en hazard-category 'foreign bodies' en Fold 4: {'classifier__alpha': 0.5, 'vectorizer__ngram_range': (1, 1)}\n",
            "Mejores hiperparámetros para predecir 'product-category' en hazard-category 'chemical' en Fold 4: {'classifier__alpha': 0.3, 'vectorizer__ngram_range': (1, 1)}\n",
            "Mejores hiperparámetros para predecir 'product-category' en hazard-category 'fraud' en Fold 4: {'classifier__alpha': 0.3, 'vectorizer__ngram_range': (1, 1)}\n",
            "Mejores hiperparámetros para predecir 'product-category' en hazard-category 'organoleptic aspects' en Fold 4: {'classifier__alpha': 0.5, 'vectorizer__ngram_range': (1, 3)}\n",
            "Mejores hiperparámetros para predecir 'product-category' en hazard-category 'allergens' en Fold 4: {'classifier__alpha': 0.3, 'vectorizer__ngram_range': (1, 1)}\n",
            "Mejores hiperparámetros para predecir 'product-category' en hazard-category 'packaging defect' en Fold 4: {'classifier__alpha': 0.1, 'vectorizer__ngram_range': (2, 2)}\n",
            "Mejores hiperparámetros para predecir 'product-category' en hazard-category 'other hazard' en Fold 4: {'classifier__alpha': 0.1, 'vectorizer__ngram_range': (1, 1)}\n",
            "Mejores hiperparámetros para predecir 'product-category' en hazard-category 'food additives and flavourings' en Fold 4: {'classifier__alpha': 0.1, 'vectorizer__ngram_range': (2, 2)}\n",
            "Mejores hiperparámetros para predecir 'product-category' en hazard-category 'migration' en Fold 4: {'classifier__alpha': 1.0, 'vectorizer__ngram_range': (1, 1)}\n",
            "F1-Score de 'product-category' en Fold 4: 0.4679844337438395\n",
            "---- Validación cruzada: Fold 5 ----\n",
            "Mejores hiperparámetros para predecir 'product-category' en hazard-category 'biological' en Fold 5: {'classifier__alpha': 0.1, 'vectorizer__ngram_range': (1, 1)}\n",
            "Mejores hiperparámetros para predecir 'product-category' en hazard-category 'foreign bodies' en Fold 5: {'classifier__alpha': 0.3, 'vectorizer__ngram_range': (1, 1)}\n",
            "Mejores hiperparámetros para predecir 'product-category' en hazard-category 'chemical' en Fold 5: {'classifier__alpha': 0.1, 'vectorizer__ngram_range': (1, 3)}\n",
            "Mejores hiperparámetros para predecir 'product-category' en hazard-category 'fraud' en Fold 5: {'classifier__alpha': 0.1, 'vectorizer__ngram_range': (1, 1)}\n",
            "Mejores hiperparámetros para predecir 'product-category' en hazard-category 'organoleptic aspects' en Fold 5: {'classifier__alpha': 1.0, 'vectorizer__ngram_range': (2, 2)}\n",
            "Mejores hiperparámetros para predecir 'product-category' en hazard-category 'allergens' en Fold 5: {'classifier__alpha': 0.1, 'vectorizer__ngram_range': (1, 1)}\n",
            "Mejores hiperparámetros para predecir 'product-category' en hazard-category 'packaging defect' en Fold 5: {'classifier__alpha': 0.7, 'vectorizer__ngram_range': (1, 3)}\n",
            "Mejores hiperparámetros para predecir 'product-category' en hazard-category 'other hazard' en Fold 5: {'classifier__alpha': 0.1, 'vectorizer__ngram_range': (1, 1)}\n",
            "Mejores hiperparámetros para predecir 'product-category' en hazard-category 'food additives and flavourings' en Fold 5: {'classifier__alpha': 0.1, 'vectorizer__ngram_range': (2, 2)}\n",
            "Mejores hiperparámetros para predecir 'product-category' en hazard-category 'migration' en Fold 5: {'classifier__alpha': 0.1, 'vectorizer__ngram_range': (2, 2)}\n",
            "F1-Score de 'product-category' en Fold 5: 0.46143437222039047\n",
            "\n",
            "Mejor F1-Score obtenido para 'product-category': 0.4700907849805738\n",
            "Índices de entrenamiento de la mejor partición: [   0    1    3 ... 5981 5982 5983]\n",
            "Índices de prueba de la mejor partición: [   2   20   22 ... 5949 5958 5959]\n",
            "\n",
            "---- Replicando la mejor partición manualmente ----\n",
            "\n",
            "F1-Score final de 'product-category' en la mejor partición replicada: 0.46143437222039047\n",
            "\n",
            "---- Métricas para 'hazard-category' en la mejor partición ----\n",
            "Accuracy de 'hazard-category': 0.706766917293233\n",
            "F1-Score de 'hazard-category': 0.7102080947030065\n",
            "Precision de 'hazard-category': 0.7461475215303092\n",
            "Recall de 'hazard-category': 0.706766917293233\n",
            "\n",
            "Reporte de clasificación para 'hazard-category':\n",
            "                                precision    recall  f1-score   support\n",
            "\n",
            "                     allergens       0.86      0.79      0.82       391\n",
            "                    biological       0.90      0.79      0.84       404\n",
            "                      chemical       0.36      0.77      0.49       100\n",
            "food additives and flavourings       0.00      0.00      0.00         6\n",
            "                foreign bodies       0.68      0.58      0.62       154\n",
            "                         fraud       0.36      0.57      0.44        82\n",
            "                     migration       0.00      0.00      0.00         2\n",
            "          organoleptic aspects       0.00      0.00      0.00        13\n",
            "                  other hazard       0.50      0.03      0.06        29\n",
            "              packaging defect       0.60      0.19      0.29        16\n",
            "\n",
            "                      accuracy                           0.71      1197\n",
            "                     macro avg       0.43      0.37      0.36      1197\n",
            "                  weighted avg       0.75      0.71      0.71      1197\n",
            "\n",
            "\n",
            "Matriz de confusión para 'hazard-category':\n",
            "[[309   3  23   0   4  52   0   0   0   0]\n",
            " [  8 320  67   0   9   0   0   0   0   0]\n",
            " [  5   8  77   0   8   1   0   0   0   1]\n",
            " [  0   0   4   0   1   0   0   0   1   0]\n",
            " [  9  14  22   0  89  20   0   0   0   0]\n",
            " [ 18   1  12   0   4  47   0   0   0   0]\n",
            " [  0   0   2   0   0   0   0   0   0   0]\n",
            " [  0   6   1   0   6   0   0   0   0   0]\n",
            " [  6   4   1   0   5  11   0   0   1   1]\n",
            " [  4   1   3   0   5   0   0   0   0   3]]\n"
          ]
        }
      ],
      "source": [
        "# Importar las librerías necesarias\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Importar las librerías necesarias\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, classification_report, confusion_matrix\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Paso 1: Usar 'text_clean' como entrada\n",
        "df['combined_text'] = df['text_clean']\n",
        "\n",
        "# Paso 2: Codificar las etiquetas\n",
        "le_hazard = LabelEncoder()\n",
        "df['hazard_encoded'] = le_hazard.fit_transform(df['hazard-category'])\n",
        "\n",
        "product_encoders = {}\n",
        "df['product_encoded'] = -1  # Inicializar con -1\n",
        "\n",
        "# Codificar 'product-category' dentro de cada 'hazard-category'\n",
        "hazard_categories = df['hazard-category'].unique()\n",
        "for hazard in hazard_categories:\n",
        "    mask = df['hazard-category'] == hazard\n",
        "    le_product = LabelEncoder()\n",
        "    df.loc[mask, 'product_encoded'] = le_product.fit_transform(df.loc[mask, 'product-category'])\n",
        "    product_encoders[hazard] = le_product\n",
        "\n",
        "# Paso 3: Dividir los datos para la validación cruzada\n",
        "X = df['combined_text']\n",
        "y_hazard = df['hazard_encoded']\n",
        "y_product = df['product_encoded']\n",
        "\n",
        "# Definir el pipeline base para product-category\n",
        "pipeline_product = Pipeline([\n",
        "    ('vectorizer', CountVectorizer()),\n",
        "    ('classifier', MultinomialNB())\n",
        "])\n",
        "\n",
        "# Definir los parámetros que se van a probar en la búsqueda\n",
        "param_grid = {\n",
        "    'vectorizer__ngram_range': [(1, 1), (1, 2), (2, 2), (1, 3)],  # Diferentes n-gramas\n",
        "    'classifier__alpha': [0.1, 0.3, 0.5, 0.7, 1.0],  # Parámetros de suavizado para Naive Bayes\n",
        "    # 'vectorizer__max_df': [0.75, 0.85, 1.0],  # Eliminar términos demasiado comunes\n",
        "    # 'vectorizer__min_df': [1, 5, 10]  # Eliminar términos demasiado raros\n",
        "}\n",
        "\n",
        "# Crear validación cruzada con K particiones estratificadas\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Lista para almacenar los F1-scores de cada iteración y sus índices\n",
        "f1_scores_per_fold = []\n",
        "best_train_index = None\n",
        "best_test_index = None\n",
        "best_f1_score = -1\n",
        "best_y_hazard_pred = None\n",
        "best_y_hazard_test = None\n",
        "\n",
        "# Iniciar el proceso de validación cruzada\n",
        "for fold_idx, (train_index, test_index) in enumerate(skf.split(X, y_hazard)):\n",
        "    print(f\"---- Validación cruzada: Fold {fold_idx + 1} ----\")\n",
        "\n",
        "    # Dividir los datos en entrenamiento y prueba\n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_hazard_train, y_hazard_test = y_hazard.iloc[train_index], y_hazard.iloc[test_index]\n",
        "    y_product_train, y_product_test = y_product.iloc[train_index], y_product.iloc[test_index]\n",
        "\n",
        "    # Entrenar el modelo de hazard-category\n",
        "    pipeline_hazard = Pipeline([\n",
        "        ('vectorizer', CountVectorizer()),\n",
        "        ('classifier', MultinomialNB())\n",
        "    ])\n",
        "\n",
        "    pipeline_hazard.fit(X_train, y_hazard_train)\n",
        "    y_hazard_pred = pipeline_hazard.predict(X_test)\n",
        "\n",
        "    # Ahora entrenamos y validamos el modelo jerárquico para product-category\n",
        "    product_models = {}\n",
        "\n",
        "    for hazard in hazard_categories:\n",
        "        mask_train = y_hazard_train == le_hazard.transform([hazard])[0]\n",
        "\n",
        "        X_train_hazard = X_train[mask_train]\n",
        "        y_product_train_hazard = y_product_train[mask_train]\n",
        "\n",
        "        if len(X_train_hazard) > 0:\n",
        "            # Búsqueda de hiperparámetros para product-category dentro de cada hazard-category\n",
        "            grid_search = GridSearchCV(pipeline_product, param_grid, scoring='f1_weighted', cv=5)\n",
        "            grid_search.fit(X_train_hazard, y_product_train_hazard)\n",
        "\n",
        "            product_models[hazard] = grid_search.best_estimator_\n",
        "            print(f\"Mejores hiperparámetros para predecir 'product-category' en hazard-category '{hazard}' en Fold {fold_idx + 1}: {grid_search.best_params_}\")\n",
        "\n",
        "    # Predecir product-category teniendo en cuenta hazard-category\n",
        "    y_product_pred = []\n",
        "\n",
        "    for i in range(len(X_test)):\n",
        "        hazard_idx = y_hazard_pred[i]\n",
        "        hazard_label = le_hazard.inverse_transform([hazard_idx])[0]\n",
        "\n",
        "        if hazard_label in product_models:\n",
        "            model = product_models[hazard_label]\n",
        "            X_sample = [X_test.iloc[i]]\n",
        "            product_pred_encoded = model.predict(X_sample)[0]\n",
        "            le_product = product_encoders[hazard_label]\n",
        "            product_pred_label = le_product.inverse_transform([product_pred_encoded])[0]\n",
        "            y_product_pred.append((hazard_label, product_pred_label))\n",
        "\n",
        "    # Evaluar las predicciones en este fold\n",
        "    y_product_true_labels = []\n",
        "    y_product_pred_labels = []\n",
        "\n",
        "    for i in range(len(y_product_test)):\n",
        "        hazard_idx = y_hazard_test.iloc[i]\n",
        "        hazard_label = le_hazard.inverse_transform([hazard_idx])[0]\n",
        "        product_idx = y_product_test.iloc[i]\n",
        "        le_product = product_encoders[hazard_label]\n",
        "        product_label = le_product.inverse_transform([product_idx])[0]\n",
        "\n",
        "        y_product_true_labels.append((hazard_label, product_label))\n",
        "\n",
        "        pred_hazard_label, pred_product_label = y_product_pred[i]\n",
        "        y_product_pred_labels.append((pred_hazard_label, pred_product_label))\n",
        "\n",
        "    true_labels = [label[1] for label in y_product_true_labels]\n",
        "    pred_labels = [label[1] for label in y_product_pred_labels]\n",
        "\n",
        "    # Calcular el F1-score para este fold (de product-category)\n",
        "    f1_fold = f1_score(true_labels, pred_labels, average='weighted')\n",
        "    print(f\"F1-Score de 'product-category' en Fold {fold_idx + 1}: {f1_fold}\")\n",
        "    f1_scores_per_fold.append(f1_fold)\n",
        "\n",
        "    # Si es el mejor F1-Score hasta ahora, guardar los índices y predicciones de hazard-category\n",
        "    if f1_fold > best_f1_score:\n",
        "        best_f1_score = f1_fold\n",
        "        best_train_index = train_index\n",
        "        best_test_index = test_index\n",
        "        best_y_hazard_pred = y_hazard_pred\n",
        "        best_y_hazard_test = y_hazard_test\n",
        "\n",
        "# Mostrar los resultados de los folds y la mejor partición\n",
        "print(f\"\\nMejor F1-Score obtenido para 'product-category': {best_f1_score}\")\n",
        "print(f\"Índices de entrenamiento de la mejor partición: {best_train_index}\")\n",
        "print(f\"Índices de prueba de la mejor partición: {best_test_index}\")\n",
        "\n",
        "# Paso 4: Replicar manualmente la mejor partición\n",
        "print(\"\\n---- Replicando la mejor partición manualmente ----\")\n",
        "\n",
        "# Usar los índices de la mejor partición para replicar el proceso\n",
        "X_train_best, X_test_best = X.iloc[best_train_index], X.iloc[best_test_index]\n",
        "y_hazard_train_best, y_hazard_test_best = y_hazard.iloc[best_train_index], y_hazard.iloc[best_test_index]\n",
        "y_product_train_best, y_product_test_best = y_product.iloc[best_train_index], y_product.iloc[best_test_index]\n",
        "\n",
        "# Entrenar el modelo con la mejor partición\n",
        "pipeline_hazard.fit(X_train_best, y_hazard_train_best)\n",
        "y_hazard_pred_best = pipeline_hazard.predict(X_test_best)\n",
        "\n",
        "# Entrenar y validar el modelo jerárquico para la mejor partición\n",
        "product_models_best = {}\n",
        "for hazard in hazard_categories:\n",
        "    mask_train_best = y_hazard_train_best == le_hazard.transform([hazard])[0]\n",
        "\n",
        "    X_train_hazard_best = X_train_best[mask_train_best]\n",
        "    y_product_train_hazard_best = y_product_train_best[mask_train_best]\n",
        "\n",
        "    if len(X_train_hazard_best) > 0:\n",
        "        # Búsqueda de hiperparámetros\n",
        "        grid_search = GridSearchCV(pipeline_product, param_grid, scoring='f1_weighted', cv=3)\n",
        "        grid_search.fit(X_train_hazard_best, y_product_train_hazard_best)\n",
        "\n",
        "        product_models_best[hazard] = grid_search.best_estimator_\n",
        "\n",
        "# Predecir 'product-category' con la mejor partición\n",
        "y_product_pred_best = []\n",
        "\n",
        "for i in range(len(X_test_best)):\n",
        "    hazard_idx_best = y_hazard_pred_best[i]\n",
        "    hazard_label_best = le_hazard.inverse_transform([hazard_idx_best])[0]\n",
        "\n",
        "    if hazard_label_best in product_models_best:\n",
        "        model_best = product_models_best[hazard_label_best]\n",
        "        X_sample_best = [X_test_best.iloc[i]]\n",
        "        product_pred_encoded_best = model_best.predict(X_sample_best)[0]\n",
        "        le_product_best = product_encoders[hazard_label_best]\n",
        "        product_pred_label_best = le_product_best.inverse_transform([product_pred_encoded_best])[0]\n",
        "        y_product_pred_best.append((hazard_label_best, product_pred_label_best))\n",
        "\n",
        "# Evaluar las predicciones de la mejor partición para 'product-category'\n",
        "true_labels_best = [label[1] for label in y_product_true_labels]\n",
        "pred_labels_best = [label[1] for label in y_product_pred_labels]\n",
        "\n",
        "f1_best = f1_score(true_labels_best, pred_labels_best, average='weighted')\n",
        "print(f\"\\nF1-Score final de 'product-category' en la mejor partición replicada: {f1_best}\")\n",
        "\n",
        "# ---- Añadimos las métricas para hazard-category ----\n",
        "print(\"\\n---- Métricas para 'hazard-category' en la mejor partición ----\")\n",
        "\n",
        "# Calcular las métricas de hazard-category\n",
        "accuracy_hazard = accuracy_score(best_y_hazard_test, best_y_hazard_pred)\n",
        "f1_hazard = f1_score(best_y_hazard_test, best_y_hazard_pred, average='weighted')\n",
        "precision_hazard = precision_score(best_y_hazard_test, best_y_hazard_pred, average='weighted')\n",
        "recall_hazard = recall_score(best_y_hazard_test, best_y_hazard_pred, average='weighted')\n",
        "\n",
        "# Mostrar todas las métricas\n",
        "print(f\"Accuracy de 'hazard-category': {accuracy_hazard}\")\n",
        "print(f\"F1-Score de 'hazard-category': {f1_hazard}\")\n",
        "print(f\"Precision de 'hazard-category': {precision_hazard}\")\n",
        "print(f\"Recall de 'hazard-category': {recall_hazard}\")\n",
        "print(\"\\nReporte de clasificación para 'hazard-category':\")\n",
        "print(classification_report(best_y_hazard_test, best_y_hazard_pred, target_names=le_hazard.classes_))\n",
        "\n",
        "# Mostrar la matriz de confusión de hazard-category\n",
        "print(\"\\nMatriz de confusión para 'hazard-category':\")\n",
        "print(confusion_matrix(best_y_hazard_test, best_y_hazard_pred))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
