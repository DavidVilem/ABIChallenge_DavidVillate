{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "Xuo4VwU6t1qu",
        "outputId": "c0f6e36c-4134-4995-e105-87d098d777b4"
      },
      "outputs": [],
      "source": [
        "# Importar la librería pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Especifica la ruta completa del archivo\n",
        "ruta_archivo = r'C:\\Users\\jdvil\\Documents\\Python\\NAACL\\Task_9\\starting_k\\incidents_labelled.csv'\n",
        "\n",
        "# Leer el archivo CSV\n",
        "carga = pd.read_csv(ruta_archivo, encoding='ISO-8859-15')\n",
        "\n",
        "# Mostrar las primeras filas del DataFrame\n",
        "carga.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        },
        "id": "hXZ99Sr2BF0x",
        "outputId": "31a6afc0-a6e4-45a0-f2cc-7a2d49b28817"
      },
      "outputs": [],
      "source": [
        "carga.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zejozblZ4X40"
      },
      "outputs": [],
      "source": [
        "df = carga"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4122s2ydwCoz",
        "outputId": "d2b9904c-ed70-45e5-b98f-c76b42636f91"
      },
      "outputs": [],
      "source": [
        "# Revisar datos faltantes en cada columna\n",
        "missing_data = df.isnull().sum()\n",
        "print(\"Datos faltantes por columna:\")\n",
        "print(missing_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lWRJmOhw6pa",
        "outputId": "ca473b26-7595-40db-d7bf-4c964729393c"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# Definir función para extraer caracteres especiales\n",
        "def extract_special_characters(column):\n",
        "    special_characters = re.findall(r'[^a-zA-Z0-9\\s]', ''.join(column))\n",
        "    return Counter(special_characters)\n",
        "\n",
        "# Obtener caracteres especiales y su concurrencia en las columnas 'title' y 'text'\n",
        "special_characters_title = extract_special_characters(df['title'].astype(str))\n",
        "special_characters_text = extract_special_characters(df['text'].astype(str))\n",
        "\n",
        "print(\"Caracteres especiales en la columna 'title':\")\n",
        "print(special_characters_title)\n",
        "\n",
        "print(\"\\nCaracteres especiales en la columna 'text':\")\n",
        "print(special_characters_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "_biNrsV72R24",
        "outputId": "2f990c4e-5308-4c8d-cb79-62bbd96729be"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Función para limpiar texto ignorando errores de codificación\n",
        "def clean_text(text):\n",
        "    # Intentar decodificar correctamente con UTF-8, ignorando errores\n",
        "    try:\n",
        "        # Encode the text to bytes, then decode using utf-8\n",
        "        text = text.encode('latin1', errors='ignore').decode('utf-8', errors='ignore')\n",
        "    except (UnicodeDecodeError, AttributeError):\n",
        "        pass\n",
        "    # Remover cualquier otro carácter especial no ASCII\n",
        "    return re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
        "\n",
        "# Aplicar la limpieza a las columnas 'title' y 'text'\n",
        "df['title_clean'] = df['title'].apply(clean_text)\n",
        "df['text_clean'] = df['text'].apply(clean_text)\n",
        "\n",
        "# Mostrar las primeras filas de las columnas limpiadas\n",
        "df[['title_clean', 'text_clean']].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aq3PuIL22wq6",
        "outputId": "3368b5fd-25a0-4832-84f6-71ca9c657f9a"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# Función para extraer caracteres especiales\n",
        "def extract_special_characters(column):\n",
        "    special_characters = re.findall(r'[^a-zA-Z0-9\\s]', ''.join(column))\n",
        "    return Counter(special_characters)\n",
        "\n",
        "# Revisar nuevamente los caracteres especiales en las columnas limpiadas 'title_clean' y 'text_clean'\n",
        "special_characters_title_clean = extract_special_characters(df['title_clean'].astype(str))\n",
        "special_characters_text_clean = extract_special_characters(df['text_clean'].astype(str))\n",
        "\n",
        "# Mostrar los resultados\n",
        "print(\"Caracteres especiales en la columna 'title_clean':\")\n",
        "print(special_characters_title_clean)\n",
        "\n",
        "print(\"\\nCaracteres especiales en la columna 'text_clean':\")\n",
        "print(special_characters_text_clean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "dwYr3e333Lhp",
        "outputId": "c8178d63-27d5-4d94-c2b0-c71f310b37de"
      },
      "outputs": [],
      "source": [
        "# Eliminar los saltos de línea (\\n) en las columnas 'title_clean' y 'text_clean'\n",
        "df['title_clean'] = df['title_clean'].str.replace('\\n', ' ')\n",
        "df['text_clean'] = df['text_clean'].str.replace('\\n', ' ')\n",
        "\n",
        "# Mostrar las primeras filas para verificar\n",
        "df[['title_clean', 'text_clean']].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "tDx93TAk24s-",
        "outputId": "9bd3ff56-94fd-4c77-8ff8-1c218ee30989"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HU1BEl2-37zU",
        "outputId": "27017c52-a598-445c-d12b-1b49d2060ee5"
      },
      "outputs": [],
      "source": [
        "# Revisar la distribución de las columnas objetivo: hazard-category y product-category\n",
        "hazard_distribution = df['hazard-category'].value_counts()\n",
        "product_distribution = df['product-category'].value_counts()\n",
        "\n",
        "# Mostrar la distribución\n",
        "print(\"Distribución de 'hazard-category':\")\n",
        "print(hazard_distribution)\n",
        "\n",
        "print(\"\\nDistribución de 'product-category':\")\n",
        "print(product_distribution)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_DvaFi16Ls3",
        "outputId": "83fc45cb-df73-41d5-d7a2-9f34223049f4"
      },
      "outputs": [],
      "source": [
        "# Contar cuántos hazard-category únicos tiene cada product-category\n",
        "product_hazard_mapping = df.groupby('product-category')['hazard-category'].nunique()\n",
        "\n",
        "# Filtrar los productos que están presentes en más de un hazard-category\n",
        "products_in_multiple_hazards = product_hazard_mapping[product_hazard_mapping > 1]\n",
        "\n",
        "# Mostrar los productos con más de un hazard-category\n",
        "print(\"Productos presentes en más de un hazard-category:\")\n",
        "print(products_in_multiple_hazards)\n",
        "\n",
        "# Mostrar el detalle de los productos con sus hazard-category\n",
        "for product in products_in_multiple_hazards.index:\n",
        "    print(f\"\\nProduct: {product}\")\n",
        "    # Filtrar por cada producto para ver las categorías de riesgo asociadas\n",
        "    hazards_for_product = df[df['product-category'] == product]['hazard-category'].unique()\n",
        "    print(f\"Hazard categories: {hazards_for_product}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dM6W2mqT64Wl",
        "outputId": "fed25bf3-eec5-4242-a026-c80f1d6bb518"
      },
      "outputs": [],
      "source": [
        "# Contar cuántas veces cada combinación de 'product-category' y 'hazard-category' ocurre\n",
        "product_hazard_counts = df.groupby(['product-category', 'hazard-category']).size().reset_index(name='count')\n",
        "\n",
        "# Mostrar el resultado\n",
        "print(product_hazard_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 916
        },
        "id": "6EutDvL564Ue",
        "outputId": "9197afe5-b8da-49ad-cd2e-8e8e17c71408"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Crear una tabla pivote para visualizar mejor la relación entre productos y riesgos\n",
        "pivot_table = df.groupby(['product-category', 'hazard-category']).size().unstack(fill_value=0)\n",
        "\n",
        "# Crear un heatmap con seaborn\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(pivot_table, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\n",
        "plt.title('Relación entre Product Category y Hazard Category')\n",
        "plt.ylabel('Product Category')\n",
        "plt.xlabel('Hazard Category')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-bs5inT8hsG",
        "outputId": "b0eaf5a1-844c-458d-9f48-b5c5d1919ac1"
      },
      "outputs": [],
      "source": [
        "# Obtener los países únicos en la columna 'country'\n",
        "unique_countries = df['country'].unique()\n",
        "\n",
        "# Mostrar la lista de países únicos\n",
        "print(\"Países únicos en el DataFrame:\")\n",
        "print(unique_countries)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYHMkqbD8PSR",
        "outputId": "4e9fd961-310a-45e0-b972-45a2b2f7a70e"
      },
      "outputs": [],
      "source": [
        "# Agrupar por year, country, y product-category, luego contar los hazard-category únicos\n",
        "product_hazard_per_location = df.groupby(['year', 'country', 'product-category'])['hazard-category'].nunique().reset_index()\n",
        "\n",
        "# Filtrar aquellos casos donde un product-category está presente en más de un hazard-category\n",
        "products_in_multiple_hazards_location = product_hazard_per_location[product_hazard_per_location['hazard-category'] > 1]\n",
        "\n",
        "# Mostrar los resultados\n",
        "print(\"Productos que están presentes en múltiples hazard-category en la misma combinación de año y país:\")\n",
        "print(products_in_multiple_hazards_location)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "id": "c_R5n3Sq8PQN",
        "outputId": "293e47e5-388b-4833-fbac-d614f4f9880b"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Crear un gráfico de barras por país\n",
        "country_summary = products_in_multiple_hazards_location.groupby('country').size().reset_index(name='num_products')\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.bar(country_summary['country'], country_summary['num_products'])\n",
        "plt.title('Número de productos en múltiples hazard-category por país')\n",
        "plt.xlabel('País')\n",
        "plt.ylabel('Número de productos')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OnXNkrG8PN5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSlnclwM8PLI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-9ITtHu64SU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDb3zI9T64P3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_sV4tIfj9px"
      },
      "source": [
        "# Modelo Base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaC-_nu9hVwT",
        "outputId": "74709d93-b5c9-43fc-f512-32f5fbde21b8"
      },
      "outputs": [],
      "source": [
        "# Importar las librerías necesarias\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Suponiendo que tu DataFrame se llama df\n",
        "# Paso 1: Combinar las columnas 'title_clean' y 'text_clean' en una sola\n",
        "df['combined_text'] = df['text_clean']\n",
        "\n",
        "# Paso 2: Codificar las etiquetas\n",
        "# Codificar 'hazard-category'\n",
        "le_hazard = LabelEncoder()\n",
        "df['hazard_encoded'] = le_hazard.fit_transform(df['hazard-category'])\n",
        "\n",
        "# Codificar 'product-category' dentro de cada 'hazard-category'\n",
        "# Crear un diccionario para almacenar los codificadores de 'product-category' por 'hazard-category'\n",
        "product_encoders = {}\n",
        "df['product_encoded'] = -1  # Inicializar con -1\n",
        "\n",
        "# Obtener las categorías únicas de 'hazard-category'\n",
        "hazard_categories = df['hazard-category'].unique()\n",
        "\n",
        "for hazard in hazard_categories:\n",
        "    # Filtrar el DataFrame por 'hazard-category'\n",
        "    mask = df['hazard-category'] == hazard\n",
        "    # Crear un LabelEncoder para el 'hazard-category' actual\n",
        "    le_product = LabelEncoder()\n",
        "    df.loc[mask, 'product_encoded'] = le_product.fit_transform(df.loc[mask, 'product-category'])\n",
        "    # Almacenar el codificador\n",
        "    product_encoders[hazard] = le_product\n",
        "\n",
        "# Paso 3: Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X = df['combined_text']\n",
        "y_hazard = df['hazard_encoded']\n",
        "y_product = df['product_encoded']\n",
        "\n",
        "X_train, X_test, y_hazard_train, y_hazard_test, y_product_train, y_product_test = train_test_split(\n",
        "    X, y_hazard, y_product, test_size=0.2, random_state=42, stratify=y_hazard\n",
        ")\n",
        "\n",
        "# Paso 4: Crear un Pipeline para el modelo de SVM\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "# Crear un Pipeline para 'hazard-category'\n",
        "pipeline_hazard = Pipeline([\n",
        "    ('vectorizer', CountVectorizer()),\n",
        "    ('classifier', LinearSVC())\n",
        "])\n",
        "\n",
        "# Entrenar el modelo para 'hazard-category'\n",
        "pipeline_hazard.fit(X_train, y_hazard_train)\n",
        "\n",
        "# Predecir en el conjunto de prueba\n",
        "y_hazard_pred = pipeline_hazard.predict(X_test)\n",
        "\n",
        "# Paso 5: Evaluar el modelo para 'hazard-category'\n",
        "print(\"Evaluación del modelo para 'hazard-category':\")\n",
        "print(classification_report(y_hazard_test, y_hazard_pred, target_names=le_hazard.classes_))\n",
        "print(\"Matriz de confusión:\")\n",
        "print(confusion_matrix(y_hazard_test, y_hazard_pred))\n",
        "print(\"Accuracy:\", accuracy_score(y_hazard_test, y_hazard_pred))\n",
        "print(\"F1 Score:\", f1_score(y_hazard_test, y_hazard_pred, average='weighted'))\n",
        "print(\"Precision:\", precision_score(y_hazard_test, y_hazard_pred, average='weighted'))\n",
        "print(\"Recall:\", recall_score(y_hazard_test, y_hazard_pred, average='weighted'))\n",
        "\n",
        "# Paso 6: Para 'product-category', necesitamos tener en cuenta la jerarquía\n",
        "# Crear un diccionario para almacenar los modelos por 'hazard-category'\n",
        "product_models = {}\n",
        "\n",
        "# Entrenar un modelo para cada 'hazard-category'\n",
        "for hazard in hazard_categories:\n",
        "    # Filtrar los datos correspondientes\n",
        "    hazard_idx = le_hazard.transform([hazard])[0]\n",
        "    mask_train = y_hazard_train == hazard_idx\n",
        "    # Verificar si hay suficientes muestras para entrenar\n",
        "    if np.sum(mask_train) > 1:\n",
        "        X_train_hazard = X_train[mask_train]\n",
        "        y_product_train_hazard = y_product_train[mask_train]\n",
        "\n",
        "        # Crear y entrenar el modelo\n",
        "        pipeline_product = Pipeline([\n",
        "            ('vectorizer', CountVectorizer()),\n",
        "            ('classifier', LinearSVC())\n",
        "        ])\n",
        "        pipeline_product.fit(X_train_hazard, y_product_train_hazard)\n",
        "        # Almacenar el modelo\n",
        "        product_models[hazard] = pipeline_product\n",
        "\n",
        "# Paso 7: Predecir 'product-category' teniendo en cuenta 'hazard-category'\n",
        "y_product_pred = []\n",
        "\n",
        "for i in range(len(X_test)):\n",
        "    hazard_idx = y_hazard_pred[i]\n",
        "    hazard_label = le_hazard.inverse_transform([hazard_idx])[0]\n",
        "\n",
        "    # Si tenemos un modelo para este 'hazard-category'\n",
        "    if hazard_label in product_models:\n",
        "        model = product_models[hazard_label]\n",
        "        # Transformar el texto en una lista para el modelo\n",
        "        X_sample = [X_test.iloc[i]]\n",
        "        # Predecir 'product-category'\n",
        "        product_pred_encoded = model.predict(X_sample)[0]\n",
        "        # Decodificar la predicción\n",
        "        le_product = product_encoders[hazard_label]\n",
        "        product_pred_label = le_product.inverse_transform([product_pred_encoded])[0]\n",
        "        # Añadir a la lista de predicciones\n",
        "        y_product_pred.append((hazard_label, product_pred_label))\n",
        "    else:\n",
        "        # Si no hay modelo, asignar una etiqueta por defecto\n",
        "        y_product_pred.append((hazard_label, None))\n",
        "\n",
        "# Paso 8: Evaluar el modelo para 'product-category'\n",
        "# Necesitamos comparar las predicciones con las etiquetas verdaderas\n",
        "y_product_true_labels = []\n",
        "y_product_pred_labels = []\n",
        "\n",
        "for i in range(len(y_product_test)):\n",
        "    hazard_idx = y_hazard_test.iloc[i]\n",
        "    hazard_label = le_hazard.inverse_transform([hazard_idx])[0]\n",
        "    product_idx = y_product_test.iloc[i]\n",
        "    le_product = product_encoders[hazard_label]\n",
        "    product_label = le_product.inverse_transform([product_idx])[0]\n",
        "\n",
        "    y_product_true_labels.append((hazard_label, product_label))\n",
        "\n",
        "    # Obtener la predicción correspondiente\n",
        "    pred_hazard_label, pred_product_label = y_product_pred[i]\n",
        "    y_product_pred_labels.append((pred_hazard_label, pred_product_label))\n",
        "\n",
        "# Evaluar solo las instancias donde tenemos predicciones válidas\n",
        "true_labels = []\n",
        "pred_labels = []\n",
        "\n",
        "for true, pred in zip(y_product_true_labels, y_product_pred_labels):\n",
        "    # Si la predicción no es None\n",
        "    if pred[1] is not None:\n",
        "        true_labels.append(true[1])\n",
        "        pred_labels.append(pred[1])\n",
        "\n",
        "# Reporte de clasificación para 'product-category'\n",
        "print(\"\\nEvaluación del modelo para 'product-category':\")\n",
        "print(classification_report(true_labels, pred_labels))\n",
        "print(\"Matriz de confusión:\")\n",
        "print(confusion_matrix(true_labels, pred_labels))\n",
        "print(\"Accuracy:\", accuracy_score(true_labels, pred_labels))\n",
        "print(\"F1 Score:\", f1_score(true_labels, pred_labels, average='weighted'))\n",
        "print(\"Precision:\", precision_score(true_labels, pred_labels, average='weighted'))\n",
        "print(\"Recall:\", recall_score(true_labels, pred_labels, average='weighted'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcjWUyii_I2H"
      },
      "source": [
        "# Mejorado Jerarquia y nuevos campos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0psjeO-H_J2_",
        "outputId": "43d8f696-8a16-4a4b-d10c-37d76169295d"
      },
      "outputs": [],
      "source": [
        "# Importar las librerías necesarias\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Paso 1: Combinar las columnas 'title_clean' y 'text_clean' en una sola\n",
        "df['combined_text'] = df['text_clean']\n",
        "\n",
        "# Paso 2: Codificar las etiquetas\n",
        "# Codificar 'hazard-category'\n",
        "le_hazard = LabelEncoder()\n",
        "df['hazard_encoded'] = le_hazard.fit_transform(df['hazard-category'])\n",
        "\n",
        "# Codificar 'product-category' dentro de cada 'hazard-category'\n",
        "# Crear un diccionario para almacenar los codificadores de 'product-category' por 'hazard-category'\n",
        "product_encoders = {}\n",
        "df['product_encoded'] = -1  # Inicializar con -1\n",
        "\n",
        "# Obtener las categorías únicas de 'hazard-category'\n",
        "hazard_categories = df['hazard-category'].unique()\n",
        "\n",
        "for hazard in hazard_categories:\n",
        "    # Filtrar el DataFrame por 'hazard-category'\n",
        "    mask = df['hazard-category'] == hazard\n",
        "    # Crear un LabelEncoder para el 'hazard-category' actual\n",
        "    le_product = LabelEncoder()\n",
        "    df.loc[mask, 'product_encoded'] = le_product.fit_transform(df.loc[mask, 'product-category'])\n",
        "    # Almacenar el codificador\n",
        "    product_encoders[hazard] = le_product\n",
        "\n",
        "# Paso 3: Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X = df['combined_text']\n",
        "y_hazard = df['hazard_encoded']\n",
        "y_product = df['product_encoded']\n",
        "\n",
        "X_train, X_test, y_hazard_train, y_hazard_test, y_product_train, y_product_test = train_test_split(\n",
        "    X, y_hazard, y_product, test_size=0.2, random_state=42, stratify=y_hazard\n",
        ")\n",
        "\n",
        "# Paso 4: Crear un Pipeline para el modelo de SVM\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "# Crear un Pipeline para 'hazard-category'\n",
        "pipeline_hazard = Pipeline([\n",
        "    ('vectorizer', CountVectorizer()),\n",
        "    ('classifier', LinearSVC())\n",
        "])\n",
        "\n",
        "# Entrenar el modelo para 'hazard-category'\n",
        "pipeline_hazard.fit(X_train, y_hazard_train)\n",
        "\n",
        "# Predecir en el conjunto de prueba\n",
        "y_hazard_pred = pipeline_hazard.predict(X_test)\n",
        "\n",
        "# Función para buscar hazard-categories válidos por año y país, retrocediendo si no hay datos\n",
        "def find_valid_hazard_categories(year, country, product_hazard_per_location):\n",
        "    while year >= product_hazard_per_location['year'].min():  # Limitar búsqueda al mínimo año disponible\n",
        "        # Filtrar por el año y país actual\n",
        "        valid_hazards = product_hazard_per_location[\n",
        "            (product_hazard_per_location['year'] == year) &\n",
        "            (product_hazard_per_location['country'] == country)\n",
        "        ]\n",
        "\n",
        "        if not valid_hazards.empty:\n",
        "            return valid_hazards['hazard-category'].unique()  # Devolver los hazard-category válidos si hay datos\n",
        "\n",
        "        year -= 1  # Retroceder al año anterior si no se encontraron datos\n",
        "\n",
        "    return None  # Si no se encuentra nada, devolver None\n",
        "\n",
        "# Función para encontrar el producto con más peso para un hazard-category dado en un año\n",
        "def find_most_frequent_product_for_hazard(hazard_label, year, country, df):\n",
        "    # Filtrar el DataFrame por el hazard-category y el año específico\n",
        "    mask = (df['hazard-category'] == hazard_label) & (df['year'] == year) & (df['country'] == country)\n",
        "    filtered_df = df[mask]\n",
        "\n",
        "    if not filtered_df.empty:\n",
        "        # Encontrar el producto más frecuente en ese hazard-category\n",
        "        most_frequent_product = filtered_df['product-category'].mode()[0]  # Devuelve el producto más frecuente\n",
        "        return most_frequent_product\n",
        "    else:\n",
        "        # Si no hay productos en ese año, retrocedemos para buscar el producto más frecuente históricamente\n",
        "        mask_all_years = df['hazard-category'] == hazard_label\n",
        "        filtered_df_all_years = df[mask_all_years]\n",
        "        if not filtered_df_all_years.empty:\n",
        "            most_frequent_product = filtered_df_all_years['product-category'].mode()[0]\n",
        "            return most_frequent_product\n",
        "        else:\n",
        "            return None  # Si no hay datos históricos, devolver None\n",
        "\n",
        "# Paso 5: Evaluar el modelo para 'hazard-category'\n",
        "print(\"Evaluación del modelo para 'hazard-category':\")\n",
        "print(classification_report(y_hazard_test, y_hazard_pred, target_names=le_hazard.classes_))\n",
        "print(\"Matriz de confusión:\")\n",
        "print(confusion_matrix(y_hazard_test, y_hazard_pred))\n",
        "print(\"Accuracy:\", accuracy_score(y_hazard_test, y_hazard_pred))\n",
        "print(\"F1 Score:\", f1_score(y_hazard_test, y_hazard_pred, average='weighted'))\n",
        "print(\"Precision:\", precision_score(y_hazard_test, y_hazard_pred, average='weighted'))\n",
        "print(\"Recall:\", recall_score(y_hazard_test, y_hazard_pred, average='weighted'))\n",
        "\n",
        "# Paso 6: Para 'product-category', necesitamos tener en cuenta la jerarquía\n",
        "# Crear un diccionario para almacenar los modelos por 'hazard-category'\n",
        "product_models = {}\n",
        "\n",
        "# Entrenar un modelo para cada 'hazard-category'\n",
        "for hazard in hazard_categories:\n",
        "    # Filtrar los datos correspondientes\n",
        "    hazard_idx = le_hazard.transform([hazard])[0]\n",
        "    mask_train = y_hazard_train == hazard_idx\n",
        "    # Verificar si hay suficientes muestras para entrenar\n",
        "    if np.sum(mask_train) > 1:\n",
        "        X_train_hazard = X_train[mask_train]\n",
        "        y_product_train_hazard = y_product_train[mask_train]\n",
        "\n",
        "        # Crear y entrenar el modelo\n",
        "        pipeline_product = Pipeline([\n",
        "            ('vectorizer', CountVectorizer()),\n",
        "            ('classifier', LinearSVC())\n",
        "        ])\n",
        "        pipeline_product.fit(X_train_hazard, y_product_train_hazard)\n",
        "        # Almacenar el modelo\n",
        "        product_models[hazard] = pipeline_product\n",
        "\n",
        "# Paso 7: Predecir 'product-category' teniendo en cuenta 'hazard-category' y validación por año/país\n",
        "y_product_pred = []\n",
        "\n",
        "# product_hazard_per_location tiene todas las combinaciones de year, country y hazard\n",
        "product_hazard_per_location = df.groupby(['year', 'country', 'product-category'])['hazard-category'].nunique().reset_index()\n",
        "\n",
        "for i in range(len(X_test)):\n",
        "    hazard_idx = y_hazard_pred[i]\n",
        "    hazard_label = le_hazard.inverse_transform([hazard_idx])[0]\n",
        "\n",
        "    # Si tenemos un modelo para este 'hazard-category'\n",
        "    if hazard_label in product_models:\n",
        "        model = product_models[hazard_label]\n",
        "        # Transformar el texto en una lista para el modelo\n",
        "        X_sample = [X_test.iloc[i]]\n",
        "        # Predecir 'product-category'\n",
        "        product_pred_encoded = model.predict(X_sample)[0]\n",
        "        # Decodificar la predicción\n",
        "        le_product = product_encoders[hazard_label]\n",
        "        product_pred_label = le_product.inverse_transform([product_pred_encoded])[0]\n",
        "        # Añadir a la lista de predicciones\n",
        "        y_product_pred.append((hazard_label, product_pred_label))\n",
        "    else:\n",
        "        # Si no hay modelo, validar año y país, y asignar un hazard válido\n",
        "        year_pred = df['year'].iloc[i]  # Año actual\n",
        "        country_pred = df['country'].iloc[i]  # País actual\n",
        "\n",
        "        valid_hazard_categories = find_valid_hazard_categories(year_pred, country_pred, product_hazard_per_location)\n",
        "\n",
        "        if valid_hazard_categories is not None:\n",
        "            # Devolver el producto con más peso para este hazard-category en ese año/país\n",
        "            most_frequent_product = find_most_frequent_product_for_hazard(hazard_label, year_pred, country_pred, df)\n",
        "            y_product_pred.append((hazard_label, most_frequent_product))\n",
        "        else:\n",
        "            # Si no se encuentran categorías válidas, asignar el producto más frecuente para ese hazard\n",
        "            most_frequent_product = find_most_frequent_product_for_hazard(hazard_label, year_pred, country_pred, df)\n",
        "            y_product_pred.append((hazard_label, most_frequent_product))\n",
        "\n",
        "# Paso 8: Evaluar el modelo para 'product-category'\n",
        "y_product_true_labels = []\n",
        "y_product_pred_labels = []\n",
        "\n",
        "for i in range(len(y_product_test)):\n",
        "    hazard_idx = y_hazard_test.iloc[i]\n",
        "    hazard_label = le_hazard.inverse_transform([hazard_idx])[0]\n",
        "    product_idx = y_product_test.iloc[i]\n",
        "    le_product = product_encoders[hazard_label]\n",
        "    product_label = le_product.inverse_transform([product_idx])[0]\n",
        "\n",
        "    y_product_true_labels.append((hazard_label, product_label))\n",
        "\n",
        "    # Obtener la predicción correspondiente\n",
        "    pred_hazard_label, pred_product_label = y_product_pred[i]\n",
        "    y_product_pred_labels.append((pred_hazard_label, pred_product_label))\n",
        "\n",
        "# Evaluar solo las instancias donde tenemos predicciones válidas\n",
        "true_labels = []\n",
        "pred_labels = []\n",
        "\n",
        "for true, pred in zip(y_product_true_labels, y_product_pred_labels):\n",
        "    # Si la predicción no es None\n",
        "    if pred[1] is not None:\n",
        "        true_labels.append(true[1])\n",
        "        pred_labels.append(pred[1])\n",
        "\n",
        "# Reporte de clasificación para 'product-category'\n",
        "print(\"\\nEvaluación del modelo para 'product-category':\")\n",
        "print(classification_report(true_labels, pred_labels))\n",
        "print(\"Matriz de confusión:\")\n",
        "print(confusion_matrix(true_labels, pred_labels))\n",
        "print(\"Accuracy:\", accuracy_score(true_labels, pred_labels))\n",
        "print(\"F1 Score:\", f1_score(true_labels, pred_labels, average='weighted'))\n",
        "print(\"Precision:\", precision_score(true_labels, pred_labels, average='weighted'))\n",
        "print(\"Recall:\", recall_score(true_labels, pred_labels, average='weighted'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQ1hFji0AHBa",
        "outputId": "b478255e-79db-4d71-f3da-4e61eb70c627"
      },
      "outputs": [],
      "source": [
        "# Crear listas para almacenar las predicciones y las etiquetas verdaderas\n",
        "hazard_category_true = []\n",
        "hazard_category_pred = []\n",
        "product_category_true = []\n",
        "product_category_pred = []\n",
        "years = []\n",
        "countries = []\n",
        "texts_clean = []\n",
        "\n",
        "# Iterar sobre el conjunto de datos de prueba y almacenar los resultados\n",
        "for i in range(len(y_product_test)):\n",
        "    # Obtener las etiquetas verdaderas para hazard-category y product-category\n",
        "    hazard_idx_true = y_hazard_test.iloc[i]\n",
        "    hazard_label_true = le_hazard.inverse_transform([hazard_idx_true])[0]\n",
        "\n",
        "    product_idx_true = y_product_test.iloc[i]\n",
        "    le_product = product_encoders[hazard_label_true]\n",
        "    product_label_true = le_product.inverse_transform([product_idx_true])[0]\n",
        "\n",
        "    # Obtener las predicciones correspondientes\n",
        "    hazard_label_pred, product_label_pred = y_product_pred[i]\n",
        "\n",
        "    # Obtener los valores adicionales\n",
        "    year = df['year'].iloc[i]\n",
        "    country = df['country'].iloc[i]\n",
        "    text_clean = df['text_clean'].iloc[i]\n",
        "\n",
        "    # Almacenar los valores en las listas\n",
        "    hazard_category_true.append(hazard_label_true)\n",
        "    hazard_category_pred.append(hazard_label_pred)\n",
        "    product_category_true.append(product_label_true)\n",
        "    product_category_pred.append(product_label_pred)\n",
        "    years.append(year)\n",
        "    countries.append(country)\n",
        "    texts_clean.append(text_clean)\n",
        "\n",
        "# Crear el DataFrame con los resultados y los campos adicionales\n",
        "df_resultados_completo = pd.DataFrame({\n",
        "    'year': years,\n",
        "    'country': countries,\n",
        "    'text_clean': texts_clean,\n",
        "    'hazard_category_true': hazard_category_true,\n",
        "    'hazard_category_pred': hazard_category_pred,\n",
        "    'product_category_true': product_category_true,\n",
        "    'product_category_pred': product_category_pred\n",
        "})\n",
        "\n",
        "# Mostrar el DataFrame de resultados completo\n",
        "print(df_resultados_completo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "r91dDtdIA8fN",
        "outputId": "9c2d7881-af7d-4c31-8dec-0db8c8f39a08"
      },
      "outputs": [],
      "source": [
        "# # Guardar el DataFrame en un archivo Excel\n",
        "# df_resultados_completo.to_excel('resultados_prediccion.xlsx', index=False)\n",
        "\n",
        "# # Descargar el archivo a tu máquina local\n",
        "# from google.colab import files\n",
        "# files.download('resultados_prediccion.xlsx')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZ02f8uWC5aM"
      },
      "source": [
        "# Modelos de Jerarquia mas COmplejo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RZLPF3aC5QP",
        "outputId": "33db0809-545f-48bf-a1b8-e8a248a3ec87"
      },
      "outputs": [],
      "source": [
        "# Importar las librerías necesarias\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "# Paso 1: Combinar las columnas 'title_clean' y 'text_clean' en una sola\n",
        "df['combined_text'] = df['text_clean']\n",
        "\n",
        "# Paso 2: Codificar las etiquetas\n",
        "# Codificar 'hazard-category'\n",
        "le_hazard = LabelEncoder()\n",
        "df['hazard_encoded'] = le_hazard.fit_transform(df['hazard-category'])\n",
        "\n",
        "# Codificar 'product-category' dentro de cada 'hazard-category'\n",
        "# Crear un diccionario para almacenar los codificadores de 'product-category' por 'hazard-category'\n",
        "product_encoders = {}\n",
        "df['product_encoded'] = -1  # Inicializar con -1\n",
        "\n",
        "# Obtener las categorías únicas de 'hazard-category'\n",
        "hazard_categories = df['hazard-category'].unique()\n",
        "\n",
        "for hazard in hazard_categories:\n",
        "    # Filtrar el DataFrame por 'hazard-category'\n",
        "    mask = df['hazard-category'] == hazard\n",
        "    # Crear un LabelEncoder para el 'hazard-category' actual\n",
        "    le_product = LabelEncoder()\n",
        "    df.loc[mask, 'product_encoded'] = le_product.fit_transform(df.loc[mask, 'product-category'])\n",
        "    # Almacenar el codificador\n",
        "    product_encoders[hazard] = le_product\n",
        "\n",
        "# Paso 3: Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X = df['combined_text']\n",
        "y_hazard = df['hazard_encoded']\n",
        "y_product = df['product_encoded']\n",
        "\n",
        "X_train, X_test, y_hazard_train, y_hazard_test, y_product_train, y_product_test = train_test_split(\n",
        "    X, y_hazard, y_product, test_size=0.2, random_state=42, stratify=y_hazard\n",
        ")\n",
        "\n",
        "# Paso 4: Crear un Pipeline para el modelo de SVM\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "# Crear un Pipeline para 'hazard-category' con TfidfVectorizer\n",
        "pipeline_hazard = Pipeline([\n",
        "    ('vectorizer', TfidfVectorizer()),\n",
        "    ('classifier', LinearSVC())\n",
        "])\n",
        "\n",
        "# Entrenar el modelo para 'hazard-category'\n",
        "pipeline_hazard.fit(X_train, y_hazard_train)\n",
        "\n",
        "# Predecir en el conjunto de prueba\n",
        "y_hazard_pred = pipeline_hazard.predict(X_test)\n",
        "\n",
        "# Función para buscar hazard-categories válidos por año y país, retrocediendo si no hay datos\n",
        "def find_valid_hazard_categories(year, country, product_hazard_per_location):\n",
        "    while year >= product_hazard_per_location['year'].min():  # Limitar búsqueda al mínimo año disponible\n",
        "        # Filtrar por el año y país actual\n",
        "        valid_hazards = product_hazard_per_location[\n",
        "            (product_hazard_per_location['year'] == year) &\n",
        "            (product_hazard_per_location['country'] == country)\n",
        "        ]\n",
        "\n",
        "        if not valid_hazards.empty:\n",
        "            return valid_hazards['hazard-category'].unique()  # Devolver los hazard-category válidos si hay datos\n",
        "\n",
        "        year -= 1  # Retroceder al año anterior si no se encontraron datos\n",
        "\n",
        "    return None  # Si no se encuentra nada, devolver None\n",
        "\n",
        "# Función para encontrar el producto con más peso para un hazard-category dado en un año\n",
        "def find_most_frequent_product_for_hazard(hazard_label, year, country, df):\n",
        "    # Filtrar el DataFrame por el hazard-category y el año específico\n",
        "    mask = (df['hazard-category'] == hazard_label) & (df['year'] == year) & (df['country'] == country)\n",
        "    filtered_df = df[mask]\n",
        "\n",
        "    if not filtered_df.empty:\n",
        "        # Encontrar el producto más frecuente en ese hazard-category\n",
        "        most_frequent_product = filtered_df['product-category'].mode()[0]  # Devuelve el producto más frecuente\n",
        "        return most_frequent_product\n",
        "    else:\n",
        "        # Si no hay productos en ese año, retrocedemos para buscar el producto más frecuente históricamente\n",
        "        mask_all_years = df['hazard-category'] == hazard_label\n",
        "        filtered_df_all_years = df[mask_all_years]\n",
        "        if not filtered_df_all_years.empty:\n",
        "            most_frequent_product = filtered_df_all_years['product-category'].mode()[0]\n",
        "            return most_frequent_product\n",
        "        else:\n",
        "            return None  # Si no hay datos históricos, devolver None\n",
        "\n",
        "# Paso 5: Evaluar el modelo para 'hazard-category'\n",
        "print(\"Evaluación del modelo para 'hazard-category':\")\n",
        "print(classification_report(y_hazard_test, y_hazard_pred, target_names=le_hazard.classes_))\n",
        "print(\"Matriz de confusión:\")\n",
        "print(confusion_matrix(y_hazard_test, y_hazard_pred))\n",
        "print(\"Accuracy:\", accuracy_score(y_hazard_test, y_hazard_pred))\n",
        "print(\"F1 Score:\", f1_score(y_hazard_test, y_hazard_pred, average='weighted'))\n",
        "print(\"Precision:\", precision_score(y_hazard_test, y_hazard_pred, average='weighted'))\n",
        "print(\"Recall:\", recall_score(y_hazard_test, y_hazard_pred, average='weighted'))\n",
        "\n",
        "# Paso 6: Para 'product-category', necesitamos tener en cuenta la jerarquía\n",
        "# Crear un diccionario para almacenar los modelos por 'hazard-category'\n",
        "product_models = {}\n",
        "\n",
        "# Entrenar un modelo para cada 'hazard-category'\n",
        "for hazard in hazard_categories:\n",
        "    # Filtrar los datos correspondientes\n",
        "    hazard_idx = le_hazard.transform([hazard])[0]\n",
        "    mask_train = y_hazard_train == hazard_idx\n",
        "    # Verificar si hay suficientes muestras para entrenar\n",
        "    if np.sum(mask_train) > 1:\n",
        "        X_train_hazard = X_train[mask_train]\n",
        "        y_product_train_hazard = y_product_train[mask_train]\n",
        "\n",
        "        # Crear y entrenar el modelo con TfidfVectorizer\n",
        "        pipeline_product = Pipeline([\n",
        "            ('vectorizer', TfidfVectorizer()),\n",
        "            ('classifier', LinearSVC())\n",
        "        ])\n",
        "        pipeline_product.fit(X_train_hazard, y_product_train_hazard)\n",
        "        # Almacenar el modelo\n",
        "        product_models[hazard] = pipeline_product\n",
        "\n",
        "# Paso 7: Predecir 'product-category' teniendo en cuenta 'hazard-category' y validación por año/país\n",
        "y_product_pred = []\n",
        "\n",
        "# product_hazard_per_location tiene todas las combinaciones de year, country y hazard\n",
        "product_hazard_per_location = df.groupby(['year', 'country', 'product-category'])['hazard-category'].nunique().reset_index()\n",
        "\n",
        "for i in range(len(X_test)):\n",
        "    hazard_idx = y_hazard_pred[i]\n",
        "    hazard_label = le_hazard.inverse_transform([hazard_idx])[0]\n",
        "\n",
        "    # Si tenemos un modelo para este 'hazard-category'\n",
        "    if hazard_label in product_models:\n",
        "        model = product_models[hazard_label]\n",
        "        # Transformar el texto en una lista para el modelo\n",
        "        X_sample = [X_test.iloc[i]]\n",
        "        # Predecir 'product-category'\n",
        "        product_pred_encoded = model.predict(X_sample)[0]\n",
        "        # Decodificar la predicción\n",
        "        le_product = product_encoders[hazard_label]\n",
        "        product_pred_label = le_product.inverse_transform([product_pred_encoded])[0]\n",
        "        # Añadir a la lista de predicciones\n",
        "        y_product_pred.append((hazard_label, product_pred_label))\n",
        "    else:\n",
        "        # Si no hay modelo, validar año y país, y asignar un hazard válido\n",
        "        year_pred = df['year'].iloc[i]  # Año actual\n",
        "        country_pred = df['country'].iloc[i]  # País actual\n",
        "\n",
        "        valid_hazard_categories = find_valid_hazard_categories(year_pred, country_pred, product_hazard_per_location)\n",
        "\n",
        "        if valid_hazard_categories is not None:\n",
        "            # Devolver el producto con más peso para este hazard-category en ese año/país\n",
        "            most_frequent_product = find_most_frequent_product_for_hazard(hazard_label, year_pred, country_pred, df)\n",
        "            y_product_pred.append((hazard_label, most_frequent_product))\n",
        "        else:\n",
        "            # Si no se encuentran categorías válidas, asignar el producto más frecuente para ese hazard\n",
        "            most_frequent_product = find_most_frequent_product_for_hazard(hazard_label, year_pred, country_pred, df)\n",
        "            y_product_pred.append((hazard_label, most_frequent_product))\n",
        "\n",
        "# Paso 8: Evaluar el modelo para 'product-category'\n",
        "y_product_true_labels = []\n",
        "y_product_pred_labels = []\n",
        "\n",
        "for i in range(len(y_product_test)):\n",
        "    hazard_idx = y_hazard_test.iloc[i]\n",
        "    hazard_label = le_hazard.inverse_transform([hazard_idx])[0]\n",
        "    product_idx = y_product_test.iloc[i]\n",
        "    le_product = product_encoders[hazard_label]\n",
        "    product_label = le_product.inverse_transform([product_idx])[0]\n",
        "\n",
        "    y_product_true_labels.append((hazard_label, product_label))\n",
        "\n",
        "    # Obtener la predicción correspondiente\n",
        "    pred_hazard_label, pred_product_label = y_product_pred[i]\n",
        "    y_product_pred_labels.append((pred_hazard_label, pred_product_label))\n",
        "\n",
        "# Evaluar solo las instancias donde tenemos predicciones válidas\n",
        "true_labels = []\n",
        "pred_labels = []\n",
        "\n",
        "for true, pred in zip(y_product_true_labels, y_product_pred_labels):\n",
        "    # Si la predicción no es None\n",
        "    if pred[1] is not None:\n",
        "        true_labels.append(true[1])\n",
        "        pred_labels.append(pred[1])\n",
        "\n",
        "# Reporte de clasificación para 'product-category'\n",
        "print(\"\\nEvaluación del modelo para 'product-category':\")\n",
        "print(classification_report(true_labels, pred_labels))\n",
        "print(\"Matriz de confusión:\")\n",
        "print(confusion_matrix(true_labels, pred_labels))\n",
        "print(\"Accuracy:\", accuracy_score(true_labels, pred_labels))\n",
        "print(\"F1 Score:\", f1_score(true_labels, pred_labels, average='weighted'))\n",
        "print(\"Precision:\", precision_score(true_labels, pred_labels, average='weighted'))\n",
        "print(\"Recall:\", recall_score(true_labels, pred_labels, average='weighted'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pP39VzKBISeo"
      },
      "source": [
        "# Hiperparametros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQ8J7zH_IUea",
        "outputId": "b0959eda-9668-4ab3-f000-9874969c4191"
      },
      "outputs": [],
      "source": [
        "# Importar las librerías necesarias\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Suponiendo que tu DataFrame se llama df\n",
        "# Paso 1: Combinar las columnas 'title_clean' y 'text_clean' en una sola\n",
        "df['combined_text'] = df['text_clean']\n",
        "\n",
        "# Paso 2: Codificar las etiquetas\n",
        "# Codificar 'hazard-category'\n",
        "le_hazard = LabelEncoder()\n",
        "df['hazard_encoded'] = le_hazard.fit_transform(df['hazard-category'])\n",
        "\n",
        "# Codificar 'product-category' dentro de cada 'hazard-category'\n",
        "# Crear un diccionario para almacenar los codificadores de 'product-category' por 'hazard-category'\n",
        "product_encoders = {}\n",
        "df['product_encoded'] = -1  # Inicializar con -1\n",
        "\n",
        "# Obtener las categorías únicas de 'hazard-category'\n",
        "hazard_categories = df['hazard-category'].unique()\n",
        "\n",
        "for hazard in hazard_categories:\n",
        "    # Filtrar el DataFrame por 'hazard-category'\n",
        "    mask = df['hazard-category'] == hazard\n",
        "    # Crear un LabelEncoder para el 'hazard-category' actual\n",
        "    le_product = LabelEncoder()\n",
        "    df.loc[mask, 'product_encoded'] = le_product.fit_transform(df.loc[mask, 'product-category'])\n",
        "    # Almacenar el codificador\n",
        "    product_encoders[hazard] = le_product\n",
        "\n",
        "# Paso 3: Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X = df['combined_text']\n",
        "y_hazard = df['hazard_encoded']\n",
        "y_product = df['product_encoded']\n",
        "\n",
        "X_train, X_test, y_hazard_train, y_hazard_test, y_product_train, y_product_test = train_test_split(\n",
        "    X, y_hazard, y_product, test_size=0.2, random_state=42, stratify=y_hazard\n",
        ")\n",
        "\n",
        "# Paso 4: Crear un Pipeline para el modelo de SVM con TfidfVectorizer y búsqueda de hiperparámetros\n",
        "pipeline_hazard = Pipeline([\n",
        "    ('vectorizer', TfidfVectorizer()),\n",
        "    ('classifier', LinearSVC())\n",
        "])\n",
        "\n",
        "# Definir el espacio de búsqueda de hiperparámetros\n",
        "param_grid_hazard = {\n",
        "    'vectorizer__max_df': [0.75, 0.85, 1.0],\n",
        "    'vectorizer__min_df': [1, 2, 5],\n",
        "    'vectorizer__ngram_range': [(1,1), (1,2)],\n",
        "    'classifier__C': [0.01, 0.1, 1, 10],\n",
        "}\n",
        "\n",
        "# Crear GridSearchCV para 'hazard-category'\n",
        "grid_search_hazard = GridSearchCV(pipeline_hazard, param_grid_hazard, cv=5, scoring='f1_weighted', verbose=1, n_jobs=-1)\n",
        "\n",
        "# Ajustar el modelo de 'hazard-category' con búsqueda de hiperparámetros\n",
        "grid_search_hazard.fit(X_train, y_hazard_train)\n",
        "\n",
        "# Obtener el mejor modelo para 'hazard-category'\n",
        "best_model_hazard = grid_search_hazard.best_estimator_\n",
        "\n",
        "# Predecir en el conjunto de prueba para 'hazard-category'\n",
        "y_hazard_pred = best_model_hazard.predict(X_test)\n",
        "\n",
        "# Evaluar el modelo de 'hazard-category'\n",
        "print(\"Evaluación del modelo para 'hazard-category' tras la optimización:\")\n",
        "print(classification_report(y_hazard_test, y_hazard_pred, target_names=le_hazard.classes_))\n",
        "\n",
        "# Paso 5: Crear un pipeline jerárquico para 'product-category'\n",
        "product_models = {}\n",
        "\n",
        "# Para cada 'hazard-category', ajustar un GridSearchCV específico para 'product-category'\n",
        "for hazard in hazard_categories:\n",
        "    hazard_idx = le_hazard.transform([hazard])[0]\n",
        "    mask_train = y_hazard_train == hazard_idx\n",
        "\n",
        "    if np.sum(mask_train) > 1:  # Verificar si hay suficientes muestras para entrenar\n",
        "        X_train_hazard = X_train[mask_train]\n",
        "        y_product_train_hazard = y_product_train[mask_train]\n",
        "\n",
        "        # Crear el pipeline para 'product-category'\n",
        "        pipeline_product = Pipeline([\n",
        "            ('vectorizer', TfidfVectorizer()),\n",
        "            ('classifier', LinearSVC())\n",
        "        ])\n",
        "\n",
        "        # Definir el espacio de búsqueda de hiperparámetros\n",
        "        param_grid_product = {\n",
        "            'vectorizer__max_df': [0.75, 0.85, 1.0],\n",
        "            'vectorizer__min_df': [1, 2, 5],\n",
        "            'vectorizer__ngram_range': [(1,1), (1,2)],\n",
        "            'classifier__C': [0.01, 0.1, 1, 10],\n",
        "        }\n",
        "\n",
        "        # Crear GridSearchCV para 'product-category'\n",
        "        grid_search_product = GridSearchCV(pipeline_product, param_grid_product, cv=5, scoring='f1_weighted', verbose=1, n_jobs=-1)\n",
        "\n",
        "        # Ajustar el modelo de 'product-category'\n",
        "        grid_search_product.fit(X_train_hazard, y_product_train_hazard)\n",
        "\n",
        "        # Almacenar el mejor modelo de 'product-category'\n",
        "        product_models[hazard] = grid_search_product.best_estimator_\n",
        "\n",
        "# Paso 6: Predecir 'product-category' respetando la jerarquía\n",
        "y_product_pred = []\n",
        "\n",
        "for i in range(len(X_test)):\n",
        "    hazard_idx = y_hazard_pred[i]\n",
        "    hazard_label = le_hazard.inverse_transform([hazard_idx])[0]\n",
        "\n",
        "    if hazard_label in product_models:\n",
        "        model = product_models[hazard_label]\n",
        "        X_sample = [X_test.iloc[i]]\n",
        "        product_pred_encoded = model.predict(X_sample)[0]\n",
        "        le_product = product_encoders[hazard_label]\n",
        "        product_pred_label = le_product.inverse_transform([product_pred_encoded])[0]\n",
        "        y_product_pred.append((hazard_label, product_pred_label))\n",
        "    else:\n",
        "        y_product_pred.append((hazard_label, None))\n",
        "\n",
        "# Evaluar el modelo para 'product-category'\n",
        "y_product_true_labels = []\n",
        "y_product_pred_labels = []\n",
        "\n",
        "for i in range(len(y_product_test)):\n",
        "    hazard_idx = y_hazard_test.iloc[i]\n",
        "    hazard_label = le_hazard.inverse_transform([hazard_idx])[0]\n",
        "    product_idx = y_product_test.iloc[i]\n",
        "    le_product = product_encoders[hazard_label]\n",
        "    product_label = le_product.inverse_transform([product_idx])[0]\n",
        "\n",
        "    y_product_true_labels.append((hazard_label, product_label))\n",
        "\n",
        "    pred_hazard_label, pred_product_label = y_product_pred[i]\n",
        "    y_product_pred_labels.append((pred_hazard_label, pred_product_label))\n",
        "\n",
        "# Evaluar solo las instancias donde tenemos predicciones válidas\n",
        "true_labels = []\n",
        "pred_labels = []\n",
        "\n",
        "for true, pred in zip(y_product_true_labels, y_product_pred_labels):\n",
        "    if pred[1] is not None:\n",
        "        true_labels.append(true[1])\n",
        "        pred_labels.append(pred[1])\n",
        "\n",
        "print(\"\\nEvaluación del modelo para 'product-category' tras la optimización:\")\n",
        "print(classification_report(true_labels, pred_labels))\n",
        "print(\"Matriz de confusión:\")\n",
        "print(confusion_matrix(true_labels, pred_labels))\n",
        "print(\"Accuracy:\", accuracy_score(true_labels, pred_labels))\n",
        "print(\"F1 Score:\", f1_score(true_labels, pred_labels, average='weighted'))\n",
        "print(\"Precision:\", precision_score(true_labels, pred_labels, average='weighted'))\n",
        "print(\"Recall:\", recall_score(true_labels, pred_labels, average='weighted'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5li3EfBC5OO",
        "outputId": "0418b37f-5db1-4e56-cde3-1d9e1dc3bc95"
      },
      "outputs": [],
      "source": [
        "# Mostrar los mejores hiperparámetros para 'hazard-category'\n",
        "print(\"Mejores hiperparámetros para 'hazard-category':\")\n",
        "print(grid_search_hazard.best_params_)\n",
        "\n",
        "# Mostrar los mejores hiperparámetros para cada 'product-category'\n",
        "print(\"\\nMejores hiperparámetros para cada 'product-category':\")\n",
        "for hazard, model in product_models.items():\n",
        "    print(f\"Hazard-category: {hazard}\")\n",
        "    print(model.named_steps['vectorizer'], model.named_steps['classifier'])  # Mostrar el vectorizador y el clasificador\n",
        "    print(model.get_params())  # Mostrar todos los parámetros del modelo\n",
        "    print()  # Espacio entre modelos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5vMAGuvVHLH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3D4RP6nXUcsc"
      },
      "source": [
        "# Modelo Validacion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_btn_GkUeAD",
        "outputId": "a587f7b6-8b84-4d24-e8eb-c549febff4e9"
      },
      "outputs": [],
      "source": [
        "# Importar las librerías necesarias\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer  # Asegúrate de importar TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "# Paso 1: Combinar las columnas 'title_clean' y 'text_clean' en una sola\n",
        "df['combined_text'] = df['text_clean']\n",
        "\n",
        "# Paso 2: Codificar las etiquetas\n",
        "# Codificar 'hazard-category'\n",
        "le_hazard = LabelEncoder()\n",
        "df['hazard_encoded'] = le_hazard.fit_transform(df['hazard-category'])\n",
        "\n",
        "# Codificar 'product-category' dentro de cada 'hazard-category'\n",
        "# Crear un diccionario para almacenar los codificadores de 'product-category' por 'hazard-category'\n",
        "product_encoders = {}\n",
        "df['product_encoded'] = -1  # Inicializar con -1\n",
        "\n",
        "# Obtener las categorías únicas de 'hazard-category'\n",
        "hazard_categories = df['hazard-category'].unique()\n",
        "\n",
        "for hazard in hazard_categories:\n",
        "    # Filtrar el DataFrame por 'hazard-category'\n",
        "    mask = df['hazard-category'] == hazard\n",
        "    # Crear un LabelEncoder para el 'hazard-category' actual\n",
        "    le_product = LabelEncoder()\n",
        "    df.loc[mask, 'product_encoded'] = le_product.fit_transform(df.loc[mask, 'product-category'])\n",
        "    # Almacenar el codificador\n",
        "    product_encoders[hazard] = le_product\n",
        "\n",
        "# Paso 3: Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X = df['combined_text']\n",
        "y_hazard = df['hazard_encoded']\n",
        "y_product = df['product_encoded']\n",
        "\n",
        "X_train, X_test, y_hazard_train, y_hazard_test, y_product_train, y_product_test = train_test_split(\n",
        "    X, y_hazard, y_product, test_size=0.2, random_state=42, stratify=y_hazard\n",
        ")\n",
        "\n",
        "# Paso 4: Crear un Pipeline para el modelo de SVM con los hiperparámetros ajustados\n",
        "\n",
        "# Hiperparámetros obtenidos\n",
        "best_params = {\n",
        "    'classifier__C': 10,\n",
        "    'vectorizer__max_df': 0.65,\n",
        "    'vectorizer__min_df': 2,\n",
        "    # 'vectorizer__ngram_range': (1, 2)\n",
        "}\n",
        "\n",
        "# Crear el Pipeline para 'hazard-category' con TfidfVectorizer y LinearSVC usando los hiperparámetros\n",
        "pipeline_hazard = Pipeline([\n",
        "    ('vectorizer', TfidfVectorizer(\n",
        "        max_df=best_params['vectorizer__max_df'],\n",
        "        min_df=best_params['vectorizer__min_df'],\n",
        "        # ngram_range=best_params['vectorizer__ngram_range']\n",
        "    )),\n",
        "    ('classifier', LinearSVC(C=best_params['classifier__C']))\n",
        "])\n",
        "\n",
        "# Entrenar el modelo para 'hazard-category'\n",
        "pipeline_hazard.fit(X_train, y_hazard_train)\n",
        "\n",
        "# Predecir en el conjunto de prueba\n",
        "y_hazard_pred = pipeline_hazard.predict(X_test)\n",
        "\n",
        "# Función para buscar hazard-categories válidos por año y país, retrocediendo si no hay datos\n",
        "def find_valid_hazard_categories(year, country, product_hazard_per_location):\n",
        "    while year >= product_hazard_per_location['year'].min():  # Limitar búsqueda al mínimo año disponible\n",
        "        # Filtrar por el año y país actual\n",
        "        valid_hazards = product_hazard_per_location[\n",
        "            (product_hazard_per_location['year'] == year) &\n",
        "            (product_hazard_per_location['country'] == country)\n",
        "        ]\n",
        "\n",
        "        if not valid_hazards.empty:\n",
        "            return valid_hazards['hazard-category'].unique()  # Devolver los hazard-category válidos si hay datos\n",
        "\n",
        "        year -= 1  # Retroceder al año anterior si no se encontraron datos\n",
        "\n",
        "    return None  # Si no se encuentra nada, devolver None\n",
        "\n",
        "# Función para encontrar el producto con más peso para un hazard-category dado en un año\n",
        "def find_most_frequent_product_for_hazard(hazard_label, year, country, df):\n",
        "    # Filtrar el DataFrame por el hazard-category y el año específico\n",
        "    mask = (df['hazard-category'] == hazard_label) & (df['year'] == year) & (df['country'] == country)\n",
        "    filtered_df = df[mask]\n",
        "\n",
        "    if not filtered_df.empty:\n",
        "        # Encontrar el producto más frecuente en ese hazard-category\n",
        "        most_frequent_product = filtered_df['product-category'].mode()[0]  # Devuelve el producto más frecuente\n",
        "        return most_frequent_product\n",
        "    else:\n",
        "        # Si no hay productos en ese año, retrocedemos para buscar el producto más frecuente históricamente\n",
        "        mask_all_years = df['hazard-category'] == hazard_label\n",
        "        filtered_df_all_years = df[mask_all_years]\n",
        "        if not filtered_df_all_years.empty:\n",
        "            most_frequent_product = filtered_df_all_years['product-category'].mode()[0]\n",
        "            return most_frequent_product\n",
        "        else:\n",
        "            return None  # Si no hay datos históricos, devolver None\n",
        "\n",
        "# Paso 5: Evaluar el modelo para 'hazard-category'\n",
        "print(\"Evaluación del modelo para 'hazard-category':\")\n",
        "print(classification_report(y_hazard_test, y_hazard_pred, target_names=le_hazard.classes_))\n",
        "print(\"Matriz de confusión:\")\n",
        "print(confusion_matrix(y_hazard_test, y_hazard_pred))\n",
        "print(\"Accuracy:\", accuracy_score(y_hazard_test, y_hazard_pred))\n",
        "print(\"F1 Score:\", f1_score(y_hazard_test, y_hazard_pred, average='weighted'))\n",
        "print(\"Precision:\", precision_score(y_hazard_test, y_hazard_pred, average='weighted'))\n",
        "print(\"Recall:\", recall_score(y_hazard_test, y_hazard_pred, average='weighted'))\n",
        "\n",
        "# Paso 6: Para 'product-category', necesitamos tener en cuenta la jerarquía\n",
        "# Crear un diccionario para almacenar los modelos por 'hazard-category'\n",
        "product_models = {}\n",
        "\n",
        "# Entrenar un modelo para cada 'hazard-category' con los mismos hiperparámetros\n",
        "for hazard in hazard_categories:\n",
        "    # Filtrar los datos correspondientes\n",
        "    hazard_idx = le_hazard.transform([hazard])[0]\n",
        "    mask_train = y_hazard_train == hazard_idx\n",
        "    # Verificar si hay suficientes muestras para entrenar\n",
        "    if np.sum(mask_train) > 1:\n",
        "        X_train_hazard = X_train[mask_train]\n",
        "        y_product_train_hazard = y_product_train[mask_train]\n",
        "\n",
        "        # Crear y entrenar el modelo con TfidfVectorizer y LinearSVC usando los hiperparámetros\n",
        "        pipeline_product = Pipeline([\n",
        "            ('vectorizer', TfidfVectorizer(\n",
        "                max_df=best_params['vectorizer__max_df'],\n",
        "                min_df=best_params['vectorizer__min_df'],\n",
        "                # ngram_range=best_params['vectorizer__ngram_range']\n",
        "            )),\n",
        "            ('classifier', LinearSVC(C=best_params['classifier__C']))\n",
        "        ])\n",
        "        pipeline_product.fit(X_train_hazard, y_product_train_hazard)\n",
        "        # Almacenar el modelo\n",
        "        product_models[hazard] = pipeline_product\n",
        "\n",
        "# Paso 7: Predecir 'product-category' teniendo en cuenta 'hazard-category' y validación por año/país\n",
        "y_product_pred = []\n",
        "\n",
        "# product_hazard_per_location tiene todas las combinaciones de year, country y hazard\n",
        "product_hazard_per_location = df.groupby(['year', 'country', 'product-category'])['hazard-category'].nunique().reset_index()\n",
        "\n",
        "# Necesitamos los valores de 'year' y 'country' para X_test\n",
        "X_test_indices = X_test.index\n",
        "year_test = df.loc[X_test_indices, 'year'].reset_index(drop=True)\n",
        "country_test = df.loc[X_test_indices, 'country'].reset_index(drop=True)\n",
        "\n",
        "for i in range(len(X_test)):\n",
        "    hazard_idx = y_hazard_pred[i]\n",
        "    hazard_label = le_hazard.inverse_transform([hazard_idx])[0]\n",
        "\n",
        "    # Si tenemos un modelo para este 'hazard-category'\n",
        "    if hazard_label in product_models:\n",
        "        model = product_models[hazard_label]\n",
        "        # Transformar el texto en una lista para el modelo\n",
        "        X_sample = [X_test.iloc[i]]\n",
        "        # Predecir 'product-category'\n",
        "        product_pred_encoded = model.predict(X_sample)[0]\n",
        "        # Decodificar la predicción\n",
        "        le_product = product_encoders[hazard_label]\n",
        "        product_pred_label = le_product.inverse_transform([product_pred_encoded])[0]\n",
        "        # Añadir a la lista de predicciones\n",
        "        y_product_pred.append((hazard_label, product_pred_label))\n",
        "    else:\n",
        "        # Si no hay modelo, validar año y país, y asignar un hazard válido\n",
        "        year_pred = year_test.iloc[i]  # Año actual\n",
        "        country_pred = country_test.iloc[i]  # País actual\n",
        "\n",
        "        valid_hazard_categories = find_valid_hazard_categories(year_pred, country_pred, product_hazard_per_location)\n",
        "\n",
        "        if valid_hazard_categories is not None:\n",
        "            # Devolver el producto con más peso para este hazard-category en ese año/país\n",
        "            most_frequent_product = find_most_frequent_product_for_hazard(hazard_label, year_pred, country_pred, df)\n",
        "            y_product_pred.append((hazard_label, most_frequent_product))\n",
        "        else:\n",
        "            # Si no se encuentran categorías válidas, asignar el producto más frecuente para ese hazard\n",
        "            most_frequent_product = find_most_frequent_product_for_hazard(hazard_label, year_pred, country_pred, df)\n",
        "            y_product_pred.append((hazard_label, most_frequent_product))\n",
        "\n",
        "# Paso 8: Evaluar el modelo para 'product-category'\n",
        "y_product_true_labels = []\n",
        "y_product_pred_labels = []\n",
        "\n",
        "for i in range(len(y_product_test)):\n",
        "    hazard_idx = y_hazard_test.iloc[i]\n",
        "    hazard_label = le_hazard.inverse_transform([hazard_idx])[0]\n",
        "    product_idx = y_product_test.iloc[i]\n",
        "    le_product = product_encoders[hazard_label]\n",
        "    product_label = le_product.inverse_transform([product_idx])[0]\n",
        "\n",
        "    y_product_true_labels.append((hazard_label, product_label))\n",
        "\n",
        "    # Obtener la predicción correspondiente\n",
        "    pred_hazard_label, pred_product_label = y_product_pred[i]\n",
        "    y_product_pred_labels.append((pred_hazard_label, pred_product_label))\n",
        "\n",
        "# Evaluar solo las instancias donde tenemos predicciones válidas\n",
        "true_labels = []\n",
        "pred_labels = []\n",
        "\n",
        "for true, pred in zip(y_product_true_labels, y_product_pred_labels):\n",
        "    # Si la predicción no es None\n",
        "    if pred[1] is not None:\n",
        "        true_labels.append(true[1])\n",
        "        pred_labels.append(pred[1])\n",
        "\n",
        "# Reporte de clasificación para 'product-category'\n",
        "print(\"\\nEvaluación del modelo para 'product-category':\")\n",
        "print(classification_report(true_labels, pred_labels))\n",
        "print(\"Matriz de confusión:\")\n",
        "print(confusion_matrix(true_labels, pred_labels))\n",
        "print(\"Accuracy:\", accuracy_score(true_labels, pred_labels))\n",
        "print(\"F1 Score:\", f1_score(true_labels, pred_labels, average='weighted'))\n",
        "print(\"Precision:\", precision_score(true_labels, pred_labels, average='weighted'))\n",
        "print(\"Recall:\", recall_score(true_labels, pred_labels, average='weighted'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyE_L-t8C5L8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koGW4oPoC5JW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
